{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04504a9f-f096-4664-a714-ddf9c193b547",
   "metadata": {},
   "source": [
    "# Etapa 3 ‚Äì Webscraping de Fontes Complementares\n",
    "Esta etapa tem como objetivo identificar e coletar conte√∫dos t√©cnicos relevantes dispon√≠veis online que n√£o estejam presentes nos documentos em PDF j√° processados no projeto. A inclus√£o dessas fontes complementares visa enriquecer a base textual do Agente de Intelig√™ncia Artificial com informa√ß√µes mais recentes, espec√≠ficas e operacionais, oriundas de institui√ß√µes confi√°veis.\n",
    "\n",
    "**3.1 Levantamento de Fontes**\n",
    "Inicialmente, foram mapeadas p√°ginas institucionais estrat√©gicas que frequentemente publicam documentos t√©cnicos e cient√≠ficos relevantes para a sa√∫de p√∫blica. As fontes priorizadas incluem:\n",
    "\n",
    "- OPAS/OMS (Organiza√ß√£o Pan-Americana da Sa√∫de)\n",
    "\n",
    "- Minist√©rio da Sa√∫de (Brasil)\n",
    "\n",
    "- Anvisa\n",
    "\n",
    "- Fiocruz\n",
    "\n",
    "- CDC (Centers for Disease Control and Prevention ‚Äì EUA)\n",
    "\n",
    "A partir dessas p√°ginas, foram identificadas se√ß√µes com documentos atualizados, publica√ß√µes t√©cnicas, boletins informativos, relat√≥rios de monitoramento e notas normativas. Esses materiais abrangem temas como imuniza√ß√£o, doen√ßas transmiss√≠veis e n√£o transmiss√≠veis, sa√∫de mental, vigil√¢ncia laboratorial, pr√°ticas cl√≠nicas, vetores e seguran√ßa alimentar, entre outros.\n",
    "\n",
    "As informa√ß√µes levantadas foram organizadas em uma planilha com os seguintes campos:\n",
    "\n",
    "- Fonte\n",
    "\n",
    "- Link\n",
    "\n",
    "- Tipo de conte√∫do\n",
    "\n",
    "- Doen√ßa/Tema (resumido)\n",
    "\n",
    "- Observa√ß√µes\n",
    "\n",
    "**3.2 Coleta dos Conte√∫dos**\n",
    "Para automatizar a coleta dos t√≠tulos e links das publica√ß√µes mais recentes, foi desenvolvido um script em Python com uso da biblioteca BeautifulSoup, capaz de varrer automaticamente os sites indicados e extrair os principais conte√∫dos dispon√≠veis. O script realiza:\n",
    "\n",
    "- Acesso √†s p√°ginas institucionais listadas;\n",
    "\n",
    "- Extra√ß√£o dos t√≠tulos e links de documentos t√©cnicos vis√≠veis;\n",
    "\n",
    "- Organiza√ß√£o dos resultados em uma planilha .csv para posterior an√°lise e valida√ß√£o manual.\n",
    "\n",
    "*Essa abordagem permite manter a base textual atualizada com agilidade, al√©m de facilitar a inclus√£o sistem√°tica de novos documentos √† medida que forem sendo publicados.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28645537",
   "metadata": {},
   "source": [
    "## **3.1 Levantamento de Fontes**\n",
    "Inicialmente, foram mapeadas p√°ginas institucionais estrat√©gicas que frequentemente publicam documentos t√©cnicos e cient√≠ficos relevantes para a sa√∫de p√∫blica. As fontes priorizadas incluem:\n",
    "\n",
    "- OPAS/OMS (Organiza√ß√£o Pan-Americana da Sa√∫de)\n",
    "\n",
    "- Minist√©rio da Sa√∫de (Brasil)\n",
    "\n",
    "- Anvisa\n",
    "\n",
    "- Fiocruz\n",
    "\n",
    "- CDC (Centers for Disease Control and Prevention ‚Äì EUA)\n",
    "\n",
    "A partir dessas p√°ginas, foram identificadas se√ß√µes com documentos atualizados, publica√ß√µes t√©cnicas, boletins informativos, relat√≥rios de monitoramento e notas normativas. Esses materiais abrangem temas como imuniza√ß√£o, doen√ßas transmiss√≠veis e n√£o transmiss√≠veis, sa√∫de mental, vigil√¢ncia laboratorial, pr√°ticas cl√≠nicas, vetores e seguran√ßa alimentar, entre outros.\n",
    "\n",
    "As informa√ß√µes levantadas foram organizadas em uma planilha com os seguintes campos:\n",
    "\n",
    "- Fonte\n",
    "\n",
    "- Link\n",
    "\n",
    "- Tipo de conte√∫do\n",
    "\n",
    "- Doen√ßa/Tema (resumido)\n",
    "\n",
    "- Observa√ß√µes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4dc4d56a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\isisi\\anaconda3\\envs\\anaconda-toolbox\\lib\\site-packages (2.32.4)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\isisi\\anaconda3\\envs\\anaconda-toolbox\\lib\\site-packages (4.13.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\isisi\\anaconda3\\envs\\anaconda-toolbox\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\isisi\\anaconda3\\envs\\anaconda-toolbox\\lib\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\isisi\\anaconda3\\envs\\anaconda-toolbox\\lib\\site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\isisi\\anaconda3\\envs\\anaconda-toolbox\\lib\\site-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\isisi\\anaconda3\\envs\\anaconda-toolbox\\lib\\site-packages (from requests) (2025.7.14)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\isisi\\anaconda3\\envs\\anaconda-toolbox\\lib\\site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\isisi\\anaconda3\\envs\\anaconda-toolbox\\lib\\site-packages (from beautifulsoup4) (4.12.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\isisi\\anaconda3\\envs\\anaconda-toolbox\\lib\\site-packages (from pandas) (2.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\isisi\\anaconda3\\envs\\anaconda-toolbox\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\isisi\\anaconda3\\envs\\anaconda-toolbox\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\isisi\\anaconda3\\envs\\anaconda-toolbox\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\isisi\\anaconda3\\envs\\anaconda-toolbox\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pandas in c:\\users\\isisi\\anaconda3\\envs\\anaconda-toolbox\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\isisi\\anaconda3\\envs\\anaconda-toolbox\\lib\\site-packages (4.13.4)\n",
      "Requirement already satisfied: python-slugify in c:\\users\\isisi\\anaconda3\\envs\\anaconda-toolbox\\lib\\site-packages (8.0.4)\n",
      "Requirement already satisfied: openpyxl in c:\\users\\isisi\\anaconda3\\envs\\anaconda-toolbox\\lib\\site-packages (3.1.5)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\isisi\\anaconda3\\envs\\anaconda-toolbox\\lib\\site-packages (from pandas) (2.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\isisi\\anaconda3\\envs\\anaconda-toolbox\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\isisi\\anaconda3\\envs\\anaconda-toolbox\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\isisi\\anaconda3\\envs\\anaconda-toolbox\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\isisi\\anaconda3\\envs\\anaconda-toolbox\\lib\\site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\isisi\\anaconda3\\envs\\anaconda-toolbox\\lib\\site-packages (from beautifulsoup4) (4.12.2)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in c:\\users\\isisi\\anaconda3\\envs\\anaconda-toolbox\\lib\\site-packages (from python-slugify) (1.3)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\isisi\\anaconda3\\envs\\anaconda-toolbox\\lib\\site-packages (from openpyxl) (2.0.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\isisi\\anaconda3\\envs\\anaconda-toolbox\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#preparando o ambiente\n",
    "# Instala√ß√£o das bibliotecas necess√°rias\n",
    "%pip install requests beautifulsoup4 pandas\n",
    "%pip install pandas beautifulsoup4 python-slugify openpyxl\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from slugify import slugify"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef572c5d",
   "metadata": {},
   "source": [
    "### Etapa de Webscraping ‚Äì Coleta de Fontes Complementares\n",
    "\n",
    "Nesta etapa, foi realizada uma coleta automatizada de conte√∫dos a partir de p√°ginas institucionais selecionadas como fontes complementares de informa√ß√£o t√©cnica, cient√≠fica e normativa. Foram inclu√≠dos portais da OPAS, Minist√©rio da Sa√∫de, Anvisa, Fiocruz e CDC.\n",
    "\n",
    "O script acessa cada site da lista definida, extrai os links dispon√≠veis com t√≠tulos leg√≠veis e monta uma tabela com os principais resultados. Apenas links com t√≠tulos minimamente descritivos (mais de 10 caracteres) e com URLs completas s√£o considerados. Para cada site, s√£o coletados at√© 10 links relevantes.\n",
    "\n",
    "O resultado √© salvo em um arquivo CSV chamado `webscraping_fontes_complementares.csv`, contendo tr√™s colunas: `Fonte`, `T√≠tulo` e `Link`. Esse arquivo ser√° utilizado nas etapas seguintes para filtragem e classifica√ß√£o autom√°tica dos conte√∫dos coletados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f477d04-f08d-421f-9a14-21bab63bb755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Finalizado. Arquivo 'webscraping_fontes_complementares.csv' salvo com os resultados.\n"
     ]
    }
   ],
   "source": [
    "# Lista de fontes\n",
    "sites = [\n",
    "    (\"OPAS - Documentos T√©cnicos\", \"https://www.paho.org/pt/documentos-tecnicos-e-cientificos\"),\n",
    "    (\"OPAS - Not√≠cias T√©cnicas\", \"https://www.paho.org/pt/noticias/noticias-das-unidades-tecnicas\"),\n",
    "    (\"OPAS - Publica√ß√µes\", \"https://www.paho.org/pt/publicacoes\"),\n",
    "    (\"Minist√©rio da Sa√∫de - Sa√∫de A-Z\", \"https://www.gov.br/saude/pt-br/assuntos/saude-de-a-a-z\"),\n",
    "    (\"Anvisa - Not√≠cias\", \"https://www.gov.br/anvisa/pt-br/assuntos/noticias-anvisa\"),\n",
    "    (\"Fiocruz - Notas T√©cnicas\", \"https://portal.fiocruz.br/pesquisas-notas-tecnicas-e-relatorios\"),\n",
    "    (\"CDC - A-Z Topics\", \"https://www.cdc.gov/health-topics.html#cdc-atozlist\")\n",
    "]\n",
    "\n",
    "# Fun√ß√£o para extrair os links com t√≠tulos leg√≠veis\n",
    "def extrair_links(nome_fonte, url):\n",
    "    try:\n",
    "        resp = requests.get(url, timeout=15)\n",
    "        soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "        links = soup.find_all('a', href=True)\n",
    "        \n",
    "        resultados = []\n",
    "        for tag in links:\n",
    "            texto = tag.get_text(strip=True)\n",
    "            href = tag['href']\n",
    "            if texto and len(texto) > 10 and ('http' in href or href.startswith('/')):\n",
    "                link_completo = href if 'http' in href else url.split('/pt')[0] + href\n",
    "                resultados.append({'Fonte': nome_fonte, 'T√≠tulo': texto, 'Link': link_completo})\n",
    "        \n",
    "        return resultados[:10]  # Limita para 10 primeiros por site\n",
    "    except Exception as e:\n",
    "        return [{'Fonte': nome_fonte, 'T√≠tulo': f'Erro ao acessar: {e}', 'Link': url}]\n",
    "\n",
    "# Rodar scraping\n",
    "todos_resultados = []\n",
    "for nome, link in sites:\n",
    "    todos_resultados.extend(extrair_links(nome, link))\n",
    "\n",
    "# Criar DataFrame\n",
    "df = pd.DataFrame(todos_resultados)\n",
    "\n",
    "# Salvar em CSV\n",
    "df.to_csv(\"webscraping_fontes_complementares.csv\", index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(\"‚úÖ Finalizado. Arquivo 'webscraping_fontes_complementares.csv' salvo com os resultados.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08ba945-809a-44f2-bbe2-83324d2be84d",
   "metadata": {},
   "source": [
    "###  Limpeza e Classifica√ß√£o das Fontes Complementares\n",
    "\n",
    "Ap√≥s a coleta inicial dos links, esta etapa realiza a limpeza e a classifica√ß√£o autom√°tica dos conte√∫dos extra√≠dos. As a√ß√µes realizadas incluem:\n",
    "\n",
    "1. **Remo√ß√£o de registros irrelevantes**:\n",
    "   - T√≠tulos vazios ou muito curtos (menos de 10 caracteres) s√£o eliminados.\n",
    "   - Links duplicados s√£o removidos para evitar redund√¢ncias.\n",
    "   - Termos gen√©ricos ou institucionais (ex: \"fale conosco\", \"barra de governo\") s√£o filtrados para manter apenas conte√∫dos t√©cnicos relevantes.\n",
    "\n",
    "2. **Classifica√ß√£o automatizada**:\n",
    "   - Cada item √© classificado por tipo de conte√∫do: *Not√≠cia T√©cnica*, *Relat√≥rio T√©cnico*, *Publica√ß√£o T√©cnica*, *Portal de Publica√ß√µes* ou *Outro*.\n",
    "   - Tamb√©m √© feita uma classifica√ß√£o tem√°tica com base em palavras-chave, agrupando os conte√∫dos por √°reas como *Imuniza√ß√£o*, *Doen√ßas Cr√¥nicas*, *Agrot√≥xicos*, entre outras.\n",
    "\n",
    "3. **Organiza√ß√£o e exporta√ß√£o**:\n",
    "   - As colunas s√£o reorganizadas para melhor visualiza√ß√£o.\n",
    "   - O resultado final √© salvo como `fontes_complementares_classificado.xlsx`, que servir√° de base para etapas posteriores de an√°lise ou curadoria manual.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdb1e58d-580e-4a07-9623-fff71b2e3d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Planilha 'fontes_complementares_classificado.xlsx' gerada com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# Caminho para o seu arquivo CSV gerado anteriormente\n",
    "arquivo_entrada = r'C:\\Users\\isisi\\Documents\\IAVS_PROJETO\\webscraping_fontes_complementares.csv'\n",
    "arquivo_saida = r'C:\\Users\\isisi\\Documents\\IAVS_PROJETO\\fontes_complementares_classificado.xlsx'\n",
    "\n",
    "# Carregar o CSV\n",
    "df = pd.read_csv(arquivo_entrada)\n",
    "\n",
    "# 1. Remover t√≠tulos vazios ou muito curtos\n",
    "df = df[df[\"T√≠tulo\"].str.len() > 10]\n",
    "\n",
    "# 2. Remover duplicatas com base no link\n",
    "df = df.drop_duplicates(subset=\"Link\")\n",
    "\n",
    "# 3. Remover registros gen√©ricos\n",
    "palavras_excluir = [\n",
    "    \"inicial\", \"barra de governo\", \"fale conosco\", \"visite\", \"acessibilidade\",\n",
    "    \"linha do tempo\", \"galeria de\", \"curiosidades\", \"cdc.gov home\", \"other languages\",\n",
    "    \"imposto de renda\", \"atualize\"\n",
    "]\n",
    "df = df[~df[\"T√≠tulo\"].str.lower().str.contains('|'.join(palavras_excluir))]\n",
    "\n",
    "# 4. Classifica√ß√£o autom√°tica\n",
    "def classificar_tipo(texto):\n",
    "    texto = texto.lower()\n",
    "    if \"not√≠cia\" in texto or \"news\" in texto:\n",
    "        return \"Not√≠cia T√©cnica\"\n",
    "    elif \"relat√≥rio\" in texto or \"report\" in texto:\n",
    "        return \"Relat√≥rio T√©cnico\"\n",
    "    elif \"manual\" in texto or \"guia\" in texto or \"documento\" in texto:\n",
    "        return \"Publica√ß√£o T√©cnica\"\n",
    "    elif \"publica√ß√µes\" in texto or \"publications\" in texto:\n",
    "        return \"Portal de Publica√ß√µes\"\n",
    "    else:\n",
    "        return \"Outro\"\n",
    "\n",
    "def classificar_tema(texto):\n",
    "    texto = texto.lower()\n",
    "    if \"agrot√≥xico\" in texto or \"res√≠duos\" in texto:\n",
    "        return \"Agrot√≥xicos\"\n",
    "    elif \"vacina\" in texto or \"imuniza√ß√£o\" in texto:\n",
    "        return \"Imuniza√ß√£o\"\n",
    "    elif \"cardiovascular\" in texto or \"hearts\" in texto:\n",
    "        return \"Doen√ßas Cr√¥nicas\"\n",
    "    elif \"envelhecimento\" in texto:\n",
    "        return \"Envelhecimento\"\n",
    "    elif \"migra√ß√£o\" in texto:\n",
    "        return \"Sa√∫de de Popula√ß√µes Vulner√°veis\"\n",
    "    elif \"tr√¢nsito\" in texto or \"acidentes\" in texto:\n",
    "        return \"Acidentes e Viol√™ncia\"\n",
    "    else:\n",
    "        return \"Sa√∫de P√∫blica Geral\"\n",
    "\n",
    "df[\"Tipo de Conte√∫do\"] = df[\"T√≠tulo\"].apply(classificar_tipo)\n",
    "df[\"Doen√ßa/Tema\"] = df[\"T√≠tulo\"].apply(classificar_tema)\n",
    "df[\"Observa√ß√µes\"] = \"Fonte complementar automatizada ‚Äì n√£o presente nos PDFs\"\n",
    "\n",
    "# Reordenar colunas\n",
    "df_final = df[[\"Fonte\", \"T√≠tulo\", \"Link\", \"Tipo de Conte√∫do\", \"Doen√ßa/Tema\", \"Observa√ß√µes\"]]\n",
    "\n",
    "# Salvar em Excel\n",
    "df_final.to_excel(arquivo_saida, index=False)\n",
    "\n",
    "print(\"‚úÖ Planilha 'fontes_complementares_classificado.xlsx' gerada com sucesso!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee7c891-7dce-4742-91c3-7b8c4b9a912f",
   "metadata": {},
   "source": [
    "### Etapa de Extra√ß√£o e Organiza√ß√£o dos Textos Complementares\n",
    "\n",
    "**Objetivo:**  \n",
    "Extrair o conte√∫do textual das p√°ginas web listadas na etapa anterior de webscraping, convertendo automaticamente os textos em arquivos `.txt`, organizados em uma estrutura padr√£o. Esta etapa tem como finalidade alimentar o pipeline do projeto IAVS com textos limpos, classificados e organizados por fonte.\n",
    "\n",
    "**Estrutura esperada:**  \n",
    "- Uma planilha `.xlsx` com as colunas: `Fonte`, `T√≠tulo`, `Link`, `Tipo de Conte√∫do`, `Doen√ßa/Tema`, `Observa√ß√µes`.\n",
    "\n",
    "**O que foi feito:**  \n",
    "- Leitura da planilha de links coletados e classificados.\n",
    "- Remo√ß√£o de registros duplicados e conte√∫dos irrelevantes.\n",
    "- Acesso autom√°tico a cada link para extra√ß√£o do conte√∫do textual.\n",
    "- Convers√£o dos textos extra√≠dos em arquivos `.txt`.\n",
    "- Limpeza dos textos (remo√ß√£o de artefatos, espa√ßos extras, etc.).\n",
    "- Organiza√ß√£o dos arquivos em duas pastas:  \n",
    "  - `textos_complementares_raw`: vers√£o bruta extra√≠da.  \n",
    "  - `textos_complementares_limpos`: vers√£o limpa e pronta para uso.\n",
    "- Cria√ß√£o autom√°tica de todas as pastas necess√°rias, caso ainda n√£o existam.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be5eddc1-e77b-4c87-b5ab-569921536842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Lendo e classificando registros...\n",
      "Iniciando download dos textos...\n",
      "‚úÖ Texto salvo: publicacoes\n",
      "‚úÖ Texto salvo: biblioteca-digital-de-saude\n",
      "‚ö†Ô∏è Conte√∫do insuficiente: biblioteca-virtual-em-saude\n",
      "‚úÖ Texto salvo: campus-virtual-de-saude-publica\n",
      "‚úÖ Texto salvo: centros-colaboradores-da-opas-oms\n",
      "‚úÖ Texto salvo: fundos-rotatorio-regionais\n",
      "‚úÖ Texto salvo: decada-do-envelhecimento-saudavel\n",
      "‚úÖ Texto salvo: iniciativa-de-eliminacao\n",
      "‚úÖ Texto salvo: migracao-e-saude-nas-americas\n",
      "‚úÖ Texto salvo: hearts-nas-americas\n",
      "‚úÖ Texto salvo: ministerio-da-saude\n",
      "‚úÖ Texto salvo: noticias-para-os-estados\n",
      "‚úÖ Texto salvo: distrito-federal\n",
      "‚úÖ Texto salvo: espirito-santo\n",
      "‚úÖ Texto salvo: mato-grosso\n",
      "‚úÖ Texto salvo: mato-grosso-do-sul\n",
      "‚úÖ Texto salvo: minas-gerais\n",
      "‚úÖ Texto salvo: agencia-nacional-de-vigilancia-sanitaria-anvisa\n",
      "‚úÖ Texto salvo: agrotoxicos\n",
      "‚úÖ Texto salvo: disque-intoxicacao\n",
      "‚úÖ Texto salvo: monografias-de-agrotoxicos\n",
      "‚úÖ Texto salvo: programa-de-analise-de-residuos-de-agrotoxicos-em-alimentos-\n",
      "‚ö†Ô∏è Conte√∫do insuficiente: reavaliacao-de-agrotoxicos\n",
      "‚úÖ Texto salvo: legislacao-vigente\n",
      "‚ùå Erro em http://brasil.gov.br: HTTPConnectionPool(host='brasil.gov.br', port=80): Read timed out. (read timeout=15)\n",
      "‚úÖ Texto salvo: abrascao-2018\n",
      "‚úÖ Texto salvo: mosaico-fiocruz\n",
      "‚úÖ Texto salvo: centers-for-disease-control-and-prevention-cdc-twenty-four-s\n",
      "‚úÖ Texto salvo: travelers-health\n",
      "‚úÖ Texto salvo: publications\n",
      "‚úÖ Texto salvo: centers-for-disease-control-and-prevention\n",
      "‚úÖ Texto salvo: diseases-conditions\n",
      "‚úÖ Texto salvo: healthy-living\n",
      "‚úÖ Texto salvo: emergency-preparedness\n",
      "\n",
      "Limpando textos e salvando vers√£o final...\n",
      "\n",
      "Pipeline finalizado com sucesso!\n",
      "‚úîÔ∏è Arquivos crus: C:\\Users\\isisi\\Documents\\IAVS_PROJETO\\textos_complementares_raw\n",
      "‚úîÔ∏è Arquivos limpos: C:\\Users\\isisi\\Documents\\IAVS_PROJETO\\textos_convertidos\n"
     ]
    }
   ],
   "source": [
    "# === CONFIGURA√á√ÉO GERAL ===\n",
    "arquivo_excel = r'C:\\Users\\isisi\\Documents\\IAVS_PROJETO\\fontes_complementares_classificado.xlsx'\n",
    "pasta_raw = r'C:\\Users\\isisi\\Documents\\IAVS_PROJETO\\textos_complementares_raw'\n",
    "pasta_limpos = r'C:\\Users\\isisi\\Documents\\IAVS_PROJETO\\textos_convertidos'\n",
    "pasta_complementares_limpos = r'C:\\Users\\isisi\\Documents\\IAVS_PROJETO\\textos_complementares_limpos'\n",
    "# === Fun√ß√µes auxiliares ===\n",
    "\n",
    "def criar_pasta(pasta):\n",
    "    if not os.path.exists(pasta):\n",
    "        os.makedirs(pasta)\n",
    "\n",
    "def classificar_tipo(texto):\n",
    "    texto = texto.lower()\n",
    "    if \"not√≠cia\" in texto or \"news\" in texto:\n",
    "        return \"Not√≠cia T√©cnica\"\n",
    "    elif \"relat√≥rio\" in texto or \"report\" in texto:\n",
    "        return \"Relat√≥rio T√©cnico\"\n",
    "    elif \"manual\" in texto or \"guia\" in texto or \"documento\" in texto:\n",
    "        return \"Publica√ß√£o T√©cnica\"\n",
    "    elif \"publica√ß√µes\" in texto or \"publications\" in texto:\n",
    "        return \"Portal de Publica√ß√µes\"\n",
    "    else:\n",
    "        return \"Outro\"\n",
    "\n",
    "def classificar_tema(texto):\n",
    "    texto = texto.lower()\n",
    "    if \"agrot√≥xico\" in texto or \"res√≠duos\" in texto:\n",
    "        return \"Agrot√≥xicos\"\n",
    "    elif \"vacina\" in texto or \"imuniza√ß√£o\" in texto:\n",
    "        return \"Imuniza√ß√£o\"\n",
    "    elif \"cardiovascular\" in texto or \"hearts\" in texto:\n",
    "        return \"Doen√ßas Cr√¥nicas\"\n",
    "    elif \"envelhecimento\" in texto:\n",
    "        return \"Envelhecimento\"\n",
    "    elif \"migra√ß√£o\" in texto:\n",
    "        return \"Popula√ß√µes Vulner√°veis\"\n",
    "    elif \"tr√¢nsito\" in texto or \"acidentes\" in texto:\n",
    "        return \"Acidentes e Viol√™ncia\"\n",
    "    else:\n",
    "        return \"Sa√∫de P√∫blica Geral\"\n",
    "\n",
    "def limpar_texto(texto):\n",
    "    texto = re.sub(r'\\n+', '\\n', texto)\n",
    "    texto = re.sub(r'\\s+', ' ', texto)\n",
    "    return texto.strip()\n",
    "\n",
    "# === Etapa 1: Leitura e classifica√ß√£o da planilha ===\n",
    "print(\"üìä Lendo e classificando registros...\")\n",
    "df = pd.read_excel(arquivo_excel)\n",
    "\n",
    "df = df[df[\"T√≠tulo\"].str.len() > 10]\n",
    "df = df.drop_duplicates(subset=\"Link\")\n",
    "\n",
    "palavras_excluir = [\n",
    "    \"inicial\", \"barra de governo\", \"fale conosco\", \"visite\", \"acessibilidade\",\n",
    "    \"linha do tempo\", \"galeria de\", \"curiosidades\", \"cdc.gov home\", \"other languages\",\n",
    "    \"imposto de renda\", \"atualize\"\n",
    "]\n",
    "df = df[~df[\"T√≠tulo\"].str.lower().str.contains('|'.join(palavras_excluir))]\n",
    "\n",
    "df[\"Tipo de Conte√∫do\"] = df[\"T√≠tulo\"].apply(classificar_tipo)\n",
    "df[\"Doen√ßa/Tema\"] = df[\"T√≠tulo\"].apply(classificar_tema)\n",
    "df[\"Observa√ß√µes\"] = \"Fonte complementar automatizada ‚Äì n√£o presente nos PDFs\"\n",
    "df = df[[\"Fonte\", \"T√≠tulo\", \"Link\", \"Tipo de Conte√∫do\", \"Doen√ßa/Tema\", \"Observa√ß√µes\"]]\n",
    "\n",
    "# === Etapa 2: Download dos textos ===\n",
    "print(\"Iniciando download dos textos...\")\n",
    "criar_pasta(pasta_raw)\n",
    "criar_pasta(pasta_complementares_limpos)\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    fonte = slugify(row[\"Fonte\"])\n",
    "    titulo = slugify(row[\"T√≠tulo\"])[:60]\n",
    "    url = row[\"Link\"]\n",
    "    \n",
    "    pasta_fonte = os.path.join(pasta_raw, fonte)\n",
    "    criar_pasta(pasta_fonte)\n",
    "    \n",
    "    caminho_saida = os.path.join(pasta_complementares_limpos, f\"{titulo}.txt\")\n",
    "    caminho_saida2 = os.path.join(pasta_limpos, f\"{titulo}.txt\")\n",
    "    \n",
    "    try:\n",
    "        res = requests.get(url, timeout=15)\n",
    "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "        texto = \"\\n\".join([tag.get_text(strip=True) for tag in soup.find_all([\"p\", \"h1\", \"h2\", \"h3\", \"li\"])])\n",
    "        \n",
    "        if len(texto) > 100:\n",
    "            with open(caminho_saida, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(texto)\n",
    "            print(f\"‚úÖ Texto salvo: {titulo}\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Conte√∫do insuficiente: {titulo}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro em {url}: {e}\")\n",
    "\n",
    "# === Etapa 3: Limpeza dos textos ===\n",
    "print(\"\\nLimpando textos e salvando vers√£o final...\")\n",
    "criar_pasta(pasta_limpos)\n",
    "\n",
    "for fonte in os.listdir(pasta_raw):\n",
    "    pasta_fonte_raw = os.path.join(pasta_raw, fonte)\n",
    "    pasta_fonte_limpos = os.path.join(pasta_limpos, fonte)\n",
    "    criar_pasta(pasta_fonte_limpos)\n",
    "\n",
    "    for arquivo in os.listdir(pasta_fonte_raw):\n",
    "        if not arquivo.endswith(\".txt\"):\n",
    "            continue\n",
    "\n",
    "        caminho_entrada = os.path.join(pasta_fonte_raw, arquivo)\n",
    "        caminho_saida = os.path.join(pasta_fonte_limpos, arquivo)\n",
    "\n",
    "        try:\n",
    "            with open(caminho_entrada, 'r', encoding='utf-8') as f:\n",
    "                conteudo = f.read()\n",
    "\n",
    "            texto_limpo = limpar_texto(conteudo)\n",
    "\n",
    "            if len(texto_limpo) > 100:\n",
    "                with open(caminho_saida, 'w', encoding='utf-8') as f:\n",
    "                    f.write(texto_limpo)\n",
    "                print(f\"‚úÖ Texto limpo: {arquivo}\")\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è Ignorado (muito curto): {arquivo}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erro ao limpar {arquivo}: {e}\")\n",
    "\n",
    "print(\"\\nPipeline finalizado com sucesso!\")\n",
    "print(f\"‚úîÔ∏è Arquivos crus: {pasta_raw}\")\n",
    "print(f\"‚úîÔ∏è Arquivos limpos: {pasta_limpos}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-toolbox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
