{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04504a9f-f096-4664-a714-ddf9c193b547",
   "metadata": {},
   "source": [
    "# Etapa 3 – Webscraping de Fontes Complementares\n",
    "Esta etapa tem como objetivo identificar e coletar conteúdos técnicos relevantes disponíveis online que não estejam presentes nos documentos em PDF já processados no projeto. A inclusão dessas fontes complementares visa enriquecer a base textual do Agente de Inteligência Artificial com informações mais recentes, específicas e operacionais, oriundas de instituições confiáveis.\n",
    "\n",
    "**3.1 Levantamento de Fontes**\n",
    "Inicialmente, foram mapeadas páginas institucionais estratégicas que frequentemente publicam documentos técnicos e científicos relevantes para a saúde pública. As fontes priorizadas incluem:\n",
    "\n",
    "- OPAS/OMS (Organização Pan-Americana da Saúde)\n",
    "\n",
    "- Ministério da Saúde (Brasil)\n",
    "\n",
    "- Anvisa\n",
    "\n",
    "- Fiocruz\n",
    "\n",
    "- CDC (Centers for Disease Control and Prevention – EUA)\n",
    "\n",
    "A partir dessas páginas, foram identificadas seções com documentos atualizados, publicações técnicas, boletins informativos, relatórios de monitoramento e notas normativas. Esses materiais abrangem temas como imunização, doenças transmissíveis e não transmissíveis, saúde mental, vigilância laboratorial, práticas clínicas, vetores e segurança alimentar, entre outros.\n",
    "\n",
    "As informações levantadas foram organizadas em uma planilha com os seguintes campos:\n",
    "\n",
    "- Fonte\n",
    "\n",
    "- Link\n",
    "\n",
    "- Tipo de conteúdo\n",
    "\n",
    "- Doença/Tema (resumido)\n",
    "\n",
    "- Observações\n",
    "\n",
    "**3.2 Coleta dos Conteúdos**\n",
    "Para automatizar a coleta dos títulos e links das publicações mais recentes, foi desenvolvido um script em Python com uso da biblioteca BeautifulSoup, capaz de varrer automaticamente os sites indicados e extrair os principais conteúdos disponíveis. O script realiza:\n",
    "\n",
    "- Acesso às páginas institucionais listadas;\n",
    "\n",
    "- Extração dos títulos e links de documentos técnicos visíveis;\n",
    "\n",
    "- Organização dos resultados em uma planilha .csv para posterior análise e validação manual.\n",
    "\n",
    "*Essa abordagem permite manter a base textual atualizada com agilidade, além de facilitar a inclusão sistemática de novos documentos à medida que forem sendo publicados.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28645537",
   "metadata": {},
   "source": [
    "## **3.1 Levantamento de Fontes**\n",
    "Inicialmente, foram mapeadas páginas institucionais estratégicas que frequentemente publicam documentos técnicos e científicos relevantes para a saúde pública. As fontes priorizadas incluem:\n",
    "\n",
    "- OPAS/OMS (Organização Pan-Americana da Saúde)\n",
    "\n",
    "- Ministério da Saúde (Brasil)\n",
    "\n",
    "- Anvisa\n",
    "\n",
    "- Fiocruz\n",
    "\n",
    "- CDC (Centers for Disease Control and Prevention – EUA)\n",
    "\n",
    "A partir dessas páginas, foram identificadas seções com documentos atualizados, publicações técnicas, boletins informativos, relatórios de monitoramento e notas normativas. Esses materiais abrangem temas como imunização, doenças transmissíveis e não transmissíveis, saúde mental, vigilância laboratorial, práticas clínicas, vetores e segurança alimentar, entre outros.\n",
    "\n",
    "As informações levantadas foram organizadas em uma planilha com os seguintes campos:\n",
    "\n",
    "- Fonte\n",
    "\n",
    "- Link\n",
    "\n",
    "- Tipo de conteúdo\n",
    "\n",
    "- Doença/Tema (resumido)\n",
    "\n",
    "- Observações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4dc4d56a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\isisi\\anaconda3\\envs\\anaconda-toolbox\\lib\\site-packages (2.32.4)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\isisi\\anaconda3\\envs\\anaconda-toolbox\\lib\\site-packages (4.13.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\isisi\\anaconda3\\envs\\anaconda-toolbox\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\isisi\\anaconda3\\envs\\anaconda-toolbox\\lib\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\isisi\\anaconda3\\envs\\anaconda-toolbox\\lib\\site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\isisi\\anaconda3\\envs\\anaconda-toolbox\\lib\\site-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\isisi\\anaconda3\\envs\\anaconda-toolbox\\lib\\site-packages (from requests) (2025.7.14)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\isisi\\anaconda3\\envs\\anaconda-toolbox\\lib\\site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\isisi\\anaconda3\\envs\\anaconda-toolbox\\lib\\site-packages (from beautifulsoup4) (4.12.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\isisi\\anaconda3\\envs\\anaconda-toolbox\\lib\\site-packages (from pandas) (2.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\isisi\\anaconda3\\envs\\anaconda-toolbox\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\isisi\\anaconda3\\envs\\anaconda-toolbox\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\isisi\\anaconda3\\envs\\anaconda-toolbox\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\isisi\\anaconda3\\envs\\anaconda-toolbox\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pandas in c:\\users\\isisi\\anaconda3\\envs\\anaconda-toolbox\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\isisi\\anaconda3\\envs\\anaconda-toolbox\\lib\\site-packages (4.13.4)\n",
      "Requirement already satisfied: python-slugify in c:\\users\\isisi\\anaconda3\\envs\\anaconda-toolbox\\lib\\site-packages (8.0.4)\n",
      "Requirement already satisfied: openpyxl in c:\\users\\isisi\\anaconda3\\envs\\anaconda-toolbox\\lib\\site-packages (3.1.5)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\isisi\\anaconda3\\envs\\anaconda-toolbox\\lib\\site-packages (from pandas) (2.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\isisi\\anaconda3\\envs\\anaconda-toolbox\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\isisi\\anaconda3\\envs\\anaconda-toolbox\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\isisi\\anaconda3\\envs\\anaconda-toolbox\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\isisi\\anaconda3\\envs\\anaconda-toolbox\\lib\\site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\isisi\\anaconda3\\envs\\anaconda-toolbox\\lib\\site-packages (from beautifulsoup4) (4.12.2)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in c:\\users\\isisi\\anaconda3\\envs\\anaconda-toolbox\\lib\\site-packages (from python-slugify) (1.3)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\isisi\\anaconda3\\envs\\anaconda-toolbox\\lib\\site-packages (from openpyxl) (2.0.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\isisi\\anaconda3\\envs\\anaconda-toolbox\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#preparando o ambiente\n",
    "# Instalação das bibliotecas necessárias\n",
    "%pip install requests beautifulsoup4 pandas\n",
    "%pip install pandas beautifulsoup4 python-slugify openpyxl\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from slugify import slugify"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef572c5d",
   "metadata": {},
   "source": [
    "### Etapa de Webscraping – Coleta de Fontes Complementares\n",
    "\n",
    "Nesta etapa, foi realizada uma coleta automatizada de conteúdos a partir de páginas institucionais selecionadas como fontes complementares de informação técnica, científica e normativa. Foram incluídos portais da OPAS, Ministério da Saúde, Anvisa, Fiocruz e CDC.\n",
    "\n",
    "O script acessa cada site da lista definida, extrai os links disponíveis com títulos legíveis e monta uma tabela com os principais resultados. Apenas links com títulos minimamente descritivos (mais de 10 caracteres) e com URLs completas são considerados. Para cada site, são coletados até 10 links relevantes.\n",
    "\n",
    "O resultado é salvo em um arquivo CSV chamado `webscraping_fontes_complementares.csv`, contendo três colunas: `Fonte`, `Título` e `Link`. Esse arquivo será utilizado nas etapas seguintes para filtragem e classificação automática dos conteúdos coletados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f477d04-f08d-421f-9a14-21bab63bb755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finalizado. Arquivo 'webscraping_fontes_complementares.csv' salvo com os resultados.\n"
     ]
    }
   ],
   "source": [
    "# Lista de fontes\n",
    "sites = [\n",
    "    (\"OPAS - Documentos Técnicos\", \"https://www.paho.org/pt/documentos-tecnicos-e-cientificos\"),\n",
    "    (\"OPAS - Notícias Técnicas\", \"https://www.paho.org/pt/noticias/noticias-das-unidades-tecnicas\"),\n",
    "    (\"OPAS - Publicações\", \"https://www.paho.org/pt/publicacoes\"),\n",
    "    (\"Ministério da Saúde - Saúde A-Z\", \"https://www.gov.br/saude/pt-br/assuntos/saude-de-a-a-z\"),\n",
    "    (\"Anvisa - Notícias\", \"https://www.gov.br/anvisa/pt-br/assuntos/noticias-anvisa\"),\n",
    "    (\"Fiocruz - Notas Técnicas\", \"https://portal.fiocruz.br/pesquisas-notas-tecnicas-e-relatorios\"),\n",
    "    (\"CDC - A-Z Topics\", \"https://www.cdc.gov/health-topics.html#cdc-atozlist\")\n",
    "]\n",
    "\n",
    "# Função para extrair os links com títulos legíveis\n",
    "def extrair_links(nome_fonte, url):\n",
    "    try:\n",
    "        resp = requests.get(url, timeout=15)\n",
    "        soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "        links = soup.find_all('a', href=True)\n",
    "        \n",
    "        resultados = []\n",
    "        for tag in links:\n",
    "            texto = tag.get_text(strip=True)\n",
    "            href = tag['href']\n",
    "            if texto and len(texto) > 10 and ('http' in href or href.startswith('/')):\n",
    "                link_completo = href if 'http' in href else url.split('/pt')[0] + href\n",
    "                resultados.append({'Fonte': nome_fonte, 'Título': texto, 'Link': link_completo})\n",
    "        \n",
    "        return resultados[:10]  # Limita para 10 primeiros por site\n",
    "    except Exception as e:\n",
    "        return [{'Fonte': nome_fonte, 'Título': f'Erro ao acessar: {e}', 'Link': url}]\n",
    "\n",
    "# Rodar scraping\n",
    "todos_resultados = []\n",
    "for nome, link in sites:\n",
    "    todos_resultados.extend(extrair_links(nome, link))\n",
    "\n",
    "# Criar DataFrame\n",
    "df = pd.DataFrame(todos_resultados)\n",
    "\n",
    "# Salvar em CSV\n",
    "df.to_csv(\"webscraping_fontes_complementares.csv\", index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(\"✅ Finalizado. Arquivo 'webscraping_fontes_complementares.csv' salvo com os resultados.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08ba945-809a-44f2-bbe2-83324d2be84d",
   "metadata": {},
   "source": [
    "###  Limpeza e Classificação das Fontes Complementares\n",
    "\n",
    "Após a coleta inicial dos links, esta etapa realiza a limpeza e a classificação automática dos conteúdos extraídos. As ações realizadas incluem:\n",
    "\n",
    "1. **Remoção de registros irrelevantes**:\n",
    "   - Títulos vazios ou muito curtos (menos de 10 caracteres) são eliminados.\n",
    "   - Links duplicados são removidos para evitar redundâncias.\n",
    "   - Termos genéricos ou institucionais (ex: \"fale conosco\", \"barra de governo\") são filtrados para manter apenas conteúdos técnicos relevantes.\n",
    "\n",
    "2. **Classificação automatizada**:\n",
    "   - Cada item é classificado por tipo de conteúdo: *Notícia Técnica*, *Relatório Técnico*, *Publicação Técnica*, *Portal de Publicações* ou *Outro*.\n",
    "   - Também é feita uma classificação temática com base em palavras-chave, agrupando os conteúdos por áreas como *Imunização*, *Doenças Crônicas*, *Agrotóxicos*, entre outras.\n",
    "\n",
    "3. **Organização e exportação**:\n",
    "   - As colunas são reorganizadas para melhor visualização.\n",
    "   - O resultado final é salvo como `fontes_complementares_classificado.xlsx`, que servirá de base para etapas posteriores de análise ou curadoria manual.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdb1e58d-580e-4a07-9623-fff71b2e3d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Planilha 'fontes_complementares_classificado.xlsx' gerada com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# Caminho para o seu arquivo CSV gerado anteriormente\n",
    "arquivo_entrada = r'C:\\Users\\isisi\\Documents\\IAVS_PROJETO\\webscraping_fontes_complementares.csv'\n",
    "arquivo_saida = r'C:\\Users\\isisi\\Documents\\IAVS_PROJETO\\fontes_complementares_classificado.xlsx'\n",
    "\n",
    "# Carregar o CSV\n",
    "df = pd.read_csv(arquivo_entrada)\n",
    "\n",
    "# 1. Remover títulos vazios ou muito curtos\n",
    "df = df[df[\"Título\"].str.len() > 10]\n",
    "\n",
    "# 2. Remover duplicatas com base no link\n",
    "df = df.drop_duplicates(subset=\"Link\")\n",
    "\n",
    "# 3. Remover registros genéricos\n",
    "palavras_excluir = [\n",
    "    \"inicial\", \"barra de governo\", \"fale conosco\", \"visite\", \"acessibilidade\",\n",
    "    \"linha do tempo\", \"galeria de\", \"curiosidades\", \"cdc.gov home\", \"other languages\",\n",
    "    \"imposto de renda\", \"atualize\"\n",
    "]\n",
    "df = df[~df[\"Título\"].str.lower().str.contains('|'.join(palavras_excluir))]\n",
    "\n",
    "# 4. Classificação automática\n",
    "def classificar_tipo(texto):\n",
    "    texto = texto.lower()\n",
    "    if \"notícia\" in texto or \"news\" in texto:\n",
    "        return \"Notícia Técnica\"\n",
    "    elif \"relatório\" in texto or \"report\" in texto:\n",
    "        return \"Relatório Técnico\"\n",
    "    elif \"manual\" in texto or \"guia\" in texto or \"documento\" in texto:\n",
    "        return \"Publicação Técnica\"\n",
    "    elif \"publicações\" in texto or \"publications\" in texto:\n",
    "        return \"Portal de Publicações\"\n",
    "    else:\n",
    "        return \"Outro\"\n",
    "\n",
    "def classificar_tema(texto):\n",
    "    texto = texto.lower()\n",
    "    if \"agrotóxico\" in texto or \"resíduos\" in texto:\n",
    "        return \"Agrotóxicos\"\n",
    "    elif \"vacina\" in texto or \"imunização\" in texto:\n",
    "        return \"Imunização\"\n",
    "    elif \"cardiovascular\" in texto or \"hearts\" in texto:\n",
    "        return \"Doenças Crônicas\"\n",
    "    elif \"envelhecimento\" in texto:\n",
    "        return \"Envelhecimento\"\n",
    "    elif \"migração\" in texto:\n",
    "        return \"Saúde de Populações Vulneráveis\"\n",
    "    elif \"trânsito\" in texto or \"acidentes\" in texto:\n",
    "        return \"Acidentes e Violência\"\n",
    "    else:\n",
    "        return \"Saúde Pública Geral\"\n",
    "\n",
    "df[\"Tipo de Conteúdo\"] = df[\"Título\"].apply(classificar_tipo)\n",
    "df[\"Doença/Tema\"] = df[\"Título\"].apply(classificar_tema)\n",
    "df[\"Observações\"] = \"Fonte complementar automatizada – não presente nos PDFs\"\n",
    "\n",
    "# Reordenar colunas\n",
    "df_final = df[[\"Fonte\", \"Título\", \"Link\", \"Tipo de Conteúdo\", \"Doença/Tema\", \"Observações\"]]\n",
    "\n",
    "# Salvar em Excel\n",
    "df_final.to_excel(arquivo_saida, index=False)\n",
    "\n",
    "print(\"✅ Planilha 'fontes_complementares_classificado.xlsx' gerada com sucesso!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee7c891-7dce-4742-91c3-7b8c4b9a912f",
   "metadata": {},
   "source": [
    "### Etapa de Extração e Organização dos Textos Complementares\n",
    "\n",
    "**Objetivo:**  \n",
    "Extrair o conteúdo textual das páginas web listadas na etapa anterior de webscraping, convertendo automaticamente os textos em arquivos `.txt`, organizados em uma estrutura padrão. Esta etapa tem como finalidade alimentar o pipeline do projeto IAVS com textos limpos, classificados e organizados por fonte.\n",
    "\n",
    "**Estrutura esperada:**  \n",
    "- Uma planilha `.xlsx` com as colunas: `Fonte`, `Título`, `Link`, `Tipo de Conteúdo`, `Doença/Tema`, `Observações`.\n",
    "\n",
    "**O que foi feito:**  \n",
    "- Leitura da planilha de links coletados e classificados.\n",
    "- Remoção de registros duplicados e conteúdos irrelevantes.\n",
    "- Acesso automático a cada link para extração do conteúdo textual.\n",
    "- Conversão dos textos extraídos em arquivos `.txt`.\n",
    "- Limpeza dos textos (remoção de artefatos, espaços extras, etc.).\n",
    "- Organização dos arquivos em duas pastas:  \n",
    "  - `textos_complementares_raw`: versão bruta extraída.  \n",
    "  - `textos_complementares_limpos`: versão limpa e pronta para uso.\n",
    "- Criação automática de todas as pastas necessárias, caso ainda não existam.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be5eddc1-e77b-4c87-b5ab-569921536842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Lendo e classificando registros...\n",
      "Iniciando download dos textos...\n",
      "✅ Texto salvo: publicacoes\n",
      "✅ Texto salvo: biblioteca-digital-de-saude\n",
      "⚠️ Conteúdo insuficiente: biblioteca-virtual-em-saude\n",
      "✅ Texto salvo: campus-virtual-de-saude-publica\n",
      "✅ Texto salvo: centros-colaboradores-da-opas-oms\n",
      "✅ Texto salvo: fundos-rotatorio-regionais\n",
      "✅ Texto salvo: decada-do-envelhecimento-saudavel\n",
      "✅ Texto salvo: iniciativa-de-eliminacao\n",
      "✅ Texto salvo: migracao-e-saude-nas-americas\n",
      "✅ Texto salvo: hearts-nas-americas\n",
      "✅ Texto salvo: ministerio-da-saude\n",
      "✅ Texto salvo: noticias-para-os-estados\n",
      "✅ Texto salvo: distrito-federal\n",
      "✅ Texto salvo: espirito-santo\n",
      "✅ Texto salvo: mato-grosso\n",
      "✅ Texto salvo: mato-grosso-do-sul\n",
      "✅ Texto salvo: minas-gerais\n",
      "✅ Texto salvo: agencia-nacional-de-vigilancia-sanitaria-anvisa\n",
      "✅ Texto salvo: agrotoxicos\n",
      "✅ Texto salvo: disque-intoxicacao\n",
      "✅ Texto salvo: monografias-de-agrotoxicos\n",
      "✅ Texto salvo: programa-de-analise-de-residuos-de-agrotoxicos-em-alimentos-\n",
      "⚠️ Conteúdo insuficiente: reavaliacao-de-agrotoxicos\n",
      "✅ Texto salvo: legislacao-vigente\n",
      "❌ Erro em http://brasil.gov.br: HTTPConnectionPool(host='brasil.gov.br', port=80): Read timed out. (read timeout=15)\n",
      "✅ Texto salvo: abrascao-2018\n",
      "✅ Texto salvo: mosaico-fiocruz\n",
      "✅ Texto salvo: centers-for-disease-control-and-prevention-cdc-twenty-four-s\n",
      "✅ Texto salvo: travelers-health\n",
      "✅ Texto salvo: publications\n",
      "✅ Texto salvo: centers-for-disease-control-and-prevention\n",
      "✅ Texto salvo: diseases-conditions\n",
      "✅ Texto salvo: healthy-living\n",
      "✅ Texto salvo: emergency-preparedness\n",
      "\n",
      "Limpando textos e salvando versão final...\n",
      "\n",
      "Pipeline finalizado com sucesso!\n",
      "✔️ Arquivos crus: C:\\Users\\isisi\\Documents\\IAVS_PROJETO\\textos_complementares_raw\n",
      "✔️ Arquivos limpos: C:\\Users\\isisi\\Documents\\IAVS_PROJETO\\textos_convertidos\n"
     ]
    }
   ],
   "source": [
    "# === CONFIGURAÇÃO GERAL ===\n",
    "arquivo_excel = r'C:\\Users\\isisi\\Documents\\IAVS_PROJETO\\fontes_complementares_classificado.xlsx'\n",
    "pasta_raw = r'C:\\Users\\isisi\\Documents\\IAVS_PROJETO\\textos_complementares_raw'\n",
    "pasta_limpos = r'C:\\Users\\isisi\\Documents\\IAVS_PROJETO\\textos_convertidos'\n",
    "pasta_complementares_limpos = r'C:\\Users\\isisi\\Documents\\IAVS_PROJETO\\textos_complementares_limpos'\n",
    "# === Funções auxiliares ===\n",
    "\n",
    "def criar_pasta(pasta):\n",
    "    if not os.path.exists(pasta):\n",
    "        os.makedirs(pasta)\n",
    "\n",
    "def classificar_tipo(texto):\n",
    "    texto = texto.lower()\n",
    "    if \"notícia\" in texto or \"news\" in texto:\n",
    "        return \"Notícia Técnica\"\n",
    "    elif \"relatório\" in texto or \"report\" in texto:\n",
    "        return \"Relatório Técnico\"\n",
    "    elif \"manual\" in texto or \"guia\" in texto or \"documento\" in texto:\n",
    "        return \"Publicação Técnica\"\n",
    "    elif \"publicações\" in texto or \"publications\" in texto:\n",
    "        return \"Portal de Publicações\"\n",
    "    else:\n",
    "        return \"Outro\"\n",
    "\n",
    "def classificar_tema(texto):\n",
    "    texto = texto.lower()\n",
    "    if \"agrotóxico\" in texto or \"resíduos\" in texto:\n",
    "        return \"Agrotóxicos\"\n",
    "    elif \"vacina\" in texto or \"imunização\" in texto:\n",
    "        return \"Imunização\"\n",
    "    elif \"cardiovascular\" in texto or \"hearts\" in texto:\n",
    "        return \"Doenças Crônicas\"\n",
    "    elif \"envelhecimento\" in texto:\n",
    "        return \"Envelhecimento\"\n",
    "    elif \"migração\" in texto:\n",
    "        return \"Populações Vulneráveis\"\n",
    "    elif \"trânsito\" in texto or \"acidentes\" in texto:\n",
    "        return \"Acidentes e Violência\"\n",
    "    else:\n",
    "        return \"Saúde Pública Geral\"\n",
    "\n",
    "def limpar_texto(texto):\n",
    "    texto = re.sub(r'\\n+', '\\n', texto)\n",
    "    texto = re.sub(r'\\s+', ' ', texto)\n",
    "    return texto.strip()\n",
    "\n",
    "# === Etapa 1: Leitura e classificação da planilha ===\n",
    "print(\"📊 Lendo e classificando registros...\")\n",
    "df = pd.read_excel(arquivo_excel)\n",
    "\n",
    "df = df[df[\"Título\"].str.len() > 10]\n",
    "df = df.drop_duplicates(subset=\"Link\")\n",
    "\n",
    "palavras_excluir = [\n",
    "    \"inicial\", \"barra de governo\", \"fale conosco\", \"visite\", \"acessibilidade\",\n",
    "    \"linha do tempo\", \"galeria de\", \"curiosidades\", \"cdc.gov home\", \"other languages\",\n",
    "    \"imposto de renda\", \"atualize\"\n",
    "]\n",
    "df = df[~df[\"Título\"].str.lower().str.contains('|'.join(palavras_excluir))]\n",
    "\n",
    "df[\"Tipo de Conteúdo\"] = df[\"Título\"].apply(classificar_tipo)\n",
    "df[\"Doença/Tema\"] = df[\"Título\"].apply(classificar_tema)\n",
    "df[\"Observações\"] = \"Fonte complementar automatizada – não presente nos PDFs\"\n",
    "df = df[[\"Fonte\", \"Título\", \"Link\", \"Tipo de Conteúdo\", \"Doença/Tema\", \"Observações\"]]\n",
    "\n",
    "# === Etapa 2: Download dos textos ===\n",
    "print(\"Iniciando download dos textos...\")\n",
    "criar_pasta(pasta_raw)\n",
    "criar_pasta(pasta_complementares_limpos)\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    fonte = slugify(row[\"Fonte\"])\n",
    "    titulo = slugify(row[\"Título\"])[:60]\n",
    "    url = row[\"Link\"]\n",
    "    \n",
    "    pasta_fonte = os.path.join(pasta_raw, fonte)\n",
    "    criar_pasta(pasta_fonte)\n",
    "    \n",
    "    caminho_saida = os.path.join(pasta_complementares_limpos, f\"{titulo}.txt\")\n",
    "    caminho_saida2 = os.path.join(pasta_limpos, f\"{titulo}.txt\")\n",
    "    \n",
    "    try:\n",
    "        res = requests.get(url, timeout=15)\n",
    "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "        texto = \"\\n\".join([tag.get_text(strip=True) for tag in soup.find_all([\"p\", \"h1\", \"h2\", \"h3\", \"li\"])])\n",
    "        \n",
    "        if len(texto) > 100:\n",
    "            with open(caminho_saida, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(texto)\n",
    "            print(f\"✅ Texto salvo: {titulo}\")\n",
    "        else:\n",
    "            print(f\"⚠️ Conteúdo insuficiente: {titulo}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erro em {url}: {e}\")\n",
    "\n",
    "# === Etapa 3: Limpeza dos textos ===\n",
    "print(\"\\nLimpando textos e salvando versão final...\")\n",
    "criar_pasta(pasta_limpos)\n",
    "\n",
    "for fonte in os.listdir(pasta_raw):\n",
    "    pasta_fonte_raw = os.path.join(pasta_raw, fonte)\n",
    "    pasta_fonte_limpos = os.path.join(pasta_limpos, fonte)\n",
    "    criar_pasta(pasta_fonte_limpos)\n",
    "\n",
    "    for arquivo in os.listdir(pasta_fonte_raw):\n",
    "        if not arquivo.endswith(\".txt\"):\n",
    "            continue\n",
    "\n",
    "        caminho_entrada = os.path.join(pasta_fonte_raw, arquivo)\n",
    "        caminho_saida = os.path.join(pasta_fonte_limpos, arquivo)\n",
    "\n",
    "        try:\n",
    "            with open(caminho_entrada, 'r', encoding='utf-8') as f:\n",
    "                conteudo = f.read()\n",
    "\n",
    "            texto_limpo = limpar_texto(conteudo)\n",
    "\n",
    "            if len(texto_limpo) > 100:\n",
    "                with open(caminho_saida, 'w', encoding='utf-8') as f:\n",
    "                    f.write(texto_limpo)\n",
    "                print(f\"✅ Texto limpo: {arquivo}\")\n",
    "            else:\n",
    "                print(f\"⚠️ Ignorado (muito curto): {arquivo}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Erro ao limpar {arquivo}: {e}\")\n",
    "\n",
    "print(\"\\nPipeline finalizado com sucesso!\")\n",
    "print(f\"✔️ Arquivos crus: {pasta_raw}\")\n",
    "print(f\"✔️ Arquivos limpos: {pasta_limpos}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-toolbox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
