{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07868495",
   "metadata": {},
   "source": [
    "# Etapa 5 – Apoio à Avaliação da IA\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"https://img.icons8.com/ios-filled/100/artificial-intelligence.png\" width=\"80\" alt=\"Ícone IA\"/>\n",
    "</p>\n",
    "Subetapa: Filtragem dos Parágrafos para Geração de Perguntas\n",
    "\n",
    "<img src=\"https://img.icons8.com/ios-filled/50/000000/right--v1.png\" width=\"24\" style=\"vertical-align:middle; margin-right:8px;\"/> Objetivo da Subetapa\n",
    "Durante a preparação do corpus para a geração automática de perguntas, observou-se que alguns arquivos .txt, mesmo após as etapas anteriores de limpeza, ainda continham trechos irrelevantes. Esses trechos, quando enviados ao modelo de NLP, causavam lentidão no processamento e geravam perguntas mal formuladas, com baixa utilidade para fins de validação.\n",
    "\n",
    "Para resolver esse gargalo, foi desenvolvido um script de filtragem semântica automatizada. Ele identifica e remove parágrafos com conteúdo não técnico ou administrativo, como nomes de autores, contatos institucionais, fichas catalográficas, endereços e notas editoriais.\n",
    "\n",
    "**O que o Script Faz**\n",
    "O script executa as seguintes tarefas:\n",
    "\n",
    "- Lê o arquivo `paragrafos_validos_corpus.xlsx`, que contém todos os parágrafos extraídos dos arquivos .txt considerados válidos estruturalmente.\n",
    "- Aplica uma função de filtragem semântica (`paragrafo_relevante`):\n",
    "    - Remove parágrafos curtos (com menos de 6 palavras).\n",
    "    - Exclui parágrafos que contenham padrões de texto como:\n",
    "        - Nomes de autores e responsáveis técnicos;\n",
    "        - Endereços e e-mails;\n",
    "        - Informações de publicação (tiragem, edição, versão);\n",
    "        - Trechos genéricos como “anexo”, “índice”, “apêndice”;\n",
    "        - Informações institucionais repetitivas (ex: “Ministério da Saúde”).\n",
    "\n",
    "**Cria dois novos arquivos:** <img src=\"https://img.icons8.com/ios-filled/50/000000/document--v1.png\" width=\"24\" style=\"vertical-align:middle; margin-left:8px;\"/>\n",
    "\n",
    "*paragrafos_filtrados_corpus.xlsx: apenas com os parágrafos considerados relevantes.\n",
    "\n",
    "relatorio_filtragem_paragrafos.xlsx: com estatísticas da filtragem (totais e percentuais) e amostras dos parágrafos excluídos e mantidos.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96aa85f",
   "metadata": {},
   "source": [
    "# Diagnóstico Visual de Duplicados e Nomes Corrompidos\n",
    "\n",
    "Este script realiza uma varredura completa nos arquivos `.txt` localizados na pasta `corpus_final` (ou outra indicada pelo usuário), com o objetivo de identificar:\n",
    "\n",
    "- **Arquivos duplicados por conteúdo**: a partir do cálculo de hash parcial dos arquivos (1 MB inicial), o script detecta textos que possuem exatamente o mesmo conteúdo, ainda que estejam com nomes diferentes.\n",
    "- **Arquivos com nomes suspeitos ou corrompidos**: são detectadas repetições anômalas em nomes como `arquivo_txt_txt_txt_2023.txt`, sugerindo um nome mais limpo.\n",
    "\n",
    "### Funcionalidades principais\n",
    "\n",
    "- **Interface gráfica interativa**: o usuário escolhe a pasta desejada via botão e visualiza os resultados diretamente na tela.\n",
    "- **Log automático**: os resultados do diagnóstico são salvos em um arquivo `log_diagnostico.csv`, contendo os arquivos suspeitos de duplicação ou com nomes inconsistentes.\n",
    "- **Execução segura**: o script apenas **lê** os arquivos — nenhuma alteração é feita nos nomes ou conteúdos durante o diagnóstico.\n",
    "\n",
    "### Principais funções do script\n",
    "\n",
    "- `calcular_hash()`: gera uma hash para os primeiros 1MB do conteúdo do arquivo, permitindo identificar duplicações de forma eficiente.\n",
    "- `normalizar_nome()`: detecta e propõe correções para nomes de arquivos com repetições incorretas de padrões.\n",
    "- `diagnosticar_pasta()`: realiza a varredura dos arquivos e imprime os resultados na interface, além de registrar um log.\n",
    "- `iniciar_interface()`: cria a interface gráfica com campos de entrada, botões e área de visualização dos resultados.\n",
    "\n",
    "Essa etapa é essencial para garantir que a base de dados textual final não contenha arquivos redundantes nem nomes mal formatados, facilitando a organização e os próximos passos do projeto.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "694747f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\isisi\\anaconda3\\envs\\anaconda-toolbox\\lib\\site-packages (4.54.1)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\isisi\\anaconda3\\envs\\anaconda-toolbox\\lib\\site-packages (0.2.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\isisi\\anaconda3\\envs\\anaconda-toolbox\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\isisi\\anaconda3\\envs\\anaconda-toolbox\\lib\\site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\isisi\\anaconda3\\envs\\anaconda-toolbox\\lib\\site-packages (from transformers) (0.34.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\isisi\\anaconda3\\envs\\anaconda-toolbox\\lib\\site-packages (from transformers) (2.3.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\isisi\\anaconda3\\envs\\anaconda-toolbox\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\isisi\\anaconda3\\envs\\anaconda-toolbox\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\isisi\\anaconda3\\envs\\anaconda-toolbox\\lib\\site-packages (from transformers) (2025.7.34)\n",
      "Requirement already satisfied: requests in c:\\users\\isisi\\anaconda3\\envs\\anaconda-toolbox\\lib\\site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\isisi\\anaconda3\\envs\\anaconda-toolbox\\lib\\site-packages (from transformers) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\isisi\\anaconda3\\envs\\anaconda-toolbox\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\isisi\\anaconda3\\envs\\anaconda-toolbox\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.7.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\isisi\\anaconda3\\envs\\anaconda-toolbox\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\isisi\\anaconda3\\envs\\anaconda-toolbox\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\isisi\\anaconda3\\envs\\anaconda-toolbox\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\isisi\\anaconda3\\envs\\anaconda-toolbox\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\isisi\\anaconda3\\envs\\anaconda-toolbox\\lib\\site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\isisi\\anaconda3\\envs\\anaconda-toolbox\\lib\\site-packages (from requests->transformers) (2025.7.14)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#carrega as bibliotecas necessárias\n",
    "%pip install transformers sentencepiece tqdm\n",
    "\n",
    "import pandas as pd # Importa a biblioteca pandas para manipulação de dados\n",
    "import re # Importa a biblioteca re para expressões regulares\n",
    "import os # Importa a biblioteca os para interações com o sistema operacional\n",
    "import hashlib # Importa a biblioteca hashlib para calcular hashes de arquivos\n",
    "import csv # Importa a biblioteca csv para manipulação de arquivos CSV\n",
    "from pathlib import Path # Importa a classe Path da biblioteca pathlib para manipulação de caminhos de arquivos\n",
    "import tkinter as tk # Importa a biblioteca tkinter para criar interfaces gráficas\n",
    "from tkinter import filedialog, messagebox, scrolledtext # Importa componentes específicos da biblioteca tkinter para diálogos de arquivos, mensagens e caixas de texto roláveis\n",
    "from openpyxl.cell.cell import ILLEGAL_CHARACTERS_RE # Importa a expressão regular para caracteres ilegais em nomes de arquivos do openpyxl\n",
    "import torch # Importa a biblioteca torch para computação em tensores\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration # Importa o tokenizador e o modelo T5 da biblioteca transformers\n",
    "from datetime import datetime# Importa a classe datetime da biblioteca datetime para manipulação de datas e horas\n",
    "from tqdm import tqdm # Importa a biblioteca tqdm para exibir barras de progresso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef747724",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extração e limpeza de texto Diagnóstico visual de duplicados e nomes corrompidos\n",
    "# === FUNÇÕES AUXILIARES ===\n",
    "\n",
    "def calcular_hash(caminho_arquivo, limite=1024*1024):#esta etapa tem como iobjetivo calcular o hash de um arquivo para identificar duplicados\n",
    "    \"\"\"Calcula hash de uma parte do arquivo (1 MB por padrão) para diagnóstico rápido.\"\"\"\n",
    "    sha256 = hashlib.sha256()\n",
    "    with open(caminho_arquivo, 'rb') as f:\n",
    "        sha256.update(f.read(limite))\n",
    "    return sha256.hexdigest()\n",
    "\n",
    "def normalizar_nome(nome):#essa etapa busca corrigir nomes de arquivos que podem estar corrompidos ou com formatação estranha\n",
    "    \"\"\"Remove repetições consecutivas de sufixos.\"\"\"\n",
    "    padrao = r'(txt_\\w+\\d{4})+'\n",
    "    return re.sub(padrao, lambda m: m.group(1), nome)\n",
    "\n",
    "def diagnosticar_pasta(pasta_raiz, saida_texto):\n",
    "    pasta_raiz = Path(pasta_raiz)\n",
    "    hash_map = {}\n",
    "    log_linhas = []\n",
    "    log_path = pasta_raiz / \"log_diagnostico.csv\"\n",
    "\n",
    "    saida_texto.insert(tk.END, f\"🔍 Diagnóstico em: {pasta_raiz}\\n\")\n",
    "    saida_texto.insert(tk.END, \"Apenas leitura — nenhum arquivo será alterado.\\n\\n\")\n",
    "    saida_texto.update()\n",
    "\n",
    "    for caminho_arquivo in pasta_raiz.rglob(\"*.txt\"):\n",
    "        hash_arquivo = calcular_hash(caminho_arquivo)\n",
    "        nome_original = caminho_arquivo.name\n",
    "        caminho_pasta = caminho_arquivo.parent\n",
    "\n",
    "        # Verifica duplicidade de conteúdo\n",
    "        if hash_arquivo in hash_map:\n",
    "            saida_texto.insert(tk.END, f\"⚠️ Possível duplicado: {nome_original} (igual a {hash_map[hash_arquivo].name})\\n\")\n",
    "            log_linhas.append([\"DUPLICADO\", str(caminho_arquivo), \"Mesma hash de\", str(hash_map[hash_arquivo])])\n",
    "        else:\n",
    "            hash_map[hash_arquivo] = caminho_arquivo\n",
    "\n",
    "        # Verifica nome corrompido\n",
    "        nome_corrigido = normalizar_nome(nome_original)\n",
    "        if nome_corrigido != nome_original:\n",
    "            saida_texto.insert(tk.END, f\"🔁 Nome suspeito: {nome_original} → Sugerido: {nome_corrigido}\\n\")\n",
    "            log_linhas.append([\"NOME_CORROMPIDO\", str(caminho_arquivo), \"Sugerido\", nome_corrigido])\n",
    "\n",
    "    with open(log_path, mode='w', newline='', encoding='utf-8') as log_csv:\n",
    "        writer = csv.writer(log_csv)\n",
    "        writer.writerow([\"Tipo\", \"Arquivo\", \"Info\", \"Referência/Sugestão\"])\n",
    "        writer.writerows(log_linhas)\n",
    "\n",
    "    saida_texto.insert(tk.END, f\"\\n📄 Log salvo em: {log_path}\\n\")\n",
    "    saida_texto.insert(tk.END, \"✅ Diagnóstico concluído.\\n\")\n",
    "    saida_texto.update()\n",
    "\n",
    "# === INTERFACE GRÁFICA ===\n",
    "\n",
    "def iniciar_interface():\n",
    "    def escolher_pasta():\n",
    "        pasta = filedialog.askdirectory(title=\"Selecione a pasta 'corpus_final'\")\n",
    "        if pasta:\n",
    "            entrada_pasta.delete(0, tk.END)\n",
    "            entrada_pasta.insert(0, pasta)\n",
    "\n",
    "    def executar_diagnostico():\n",
    "        pasta_escolhida = entrada_pasta.get()\n",
    "        if not os.path.isdir(pasta_escolhida):\n",
    "            messagebox.showerror(\"Erro\", \"Selecione uma pasta válida.\")\n",
    "            return\n",
    "        saida_texto.delete(1.0, tk.END)\n",
    "        diagnosticar_pasta(pasta_escolhida, saida_texto)\n",
    "\n",
    "    janela = tk.Tk()\n",
    "    janela.title(\"🔎 Diagnóstico de Arquivos Duplicados e Nomes Corrompidos\")\n",
    "\n",
    "    tk.Label(janela, text=\"📁 Pasta Base:\").pack(pady=5)\n",
    "    entrada_pasta = tk.Entry(janela, width=60)\n",
    "    entrada_pasta.pack()\n",
    "    tk.Button(janela, text=\"Selecionar Pasta\", command=escolher_pasta).pack(pady=5)\n",
    "\n",
    "    tk.Button(janela, text=\"🔍 Executar Diagnóstico\", command=executar_diagnostico, bg=\"blue\", fg=\"white\").pack(pady=10)\n",
    "\n",
    "    saida_texto = scrolledtext.ScrolledText(janela, width=100, height=25)\n",
    "    saida_texto.pack(padx=10, pady=10)\n",
    "\n",
    "    janela.mainloop()\n",
    "\n",
    "# Rodar\n",
    "if __name__ == \"__main__\":\n",
    "    iniciar_interface()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c83b9c",
   "metadata": {},
   "source": [
    "#  Ações de Correção com Base no Diagnóstico\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"https://img.icons8.com/fluency/96/checked--v1.png\" width=\"80\" alt=\"Correção\"/>\n",
    "</p>\n",
    "\n",
    "Este script executa as ações sugeridas pelo diagnóstico anterior (`log_diagnostico.csv`), removendo arquivos duplicados reais e renomeando aqueles com nomes corrompidos. Ele deve ser rodado **logo após a Etapa 4**.\n",
    "\n",
    "### O que o script faz\n",
    "\n",
    "- **Lê o arquivo de log** gerado na etapa anterior (`log_diagnostico.csv`), que contém os arquivos identificados como duplicados ou com nomes suspeitos.\n",
    "- **Remove arquivos duplicados** de fato, com base no conteúdo (hash), mantendo apenas uma cópia.\n",
    "- **Renomeia arquivos** com nomes corrompidos, normalizando sufixos repetidos. Se o nome sugerido já existir, o script adiciona um sufixo incremental (`_1`, `_2`, etc.) para evitar sobrescrita.\n",
    "- **Gera um novo log (`log_execucao.csv`)**, registrando todas as ações realizadas ou ignoradas (com justificativa).\n",
    "\n",
    "### Detalhes técnicos\n",
    "\n",
    "- Os caminhos dos arquivos são tratados com `Path` para garantir compatibilidade e segurança.\n",
    "- Em caso de erro (como falha ao excluir ou renomear), o erro é capturado e registrado no log de execução.\n",
    "- O script imprime no terminal as ações realizadas para acompanhamento visual.\n",
    "\n",
    "### Exemplo de ações realizadas\n",
    "\n",
    "| Tipo           | Descrição                                               |\n",
    "|----------------|----------------------------------------------------------|\n",
    "| REMOVIDO       | Arquivo duplicado foi deletado                          |\n",
    "| RENOMEADO      | Arquivo com nome corrompido foi renomeado corretamente  |\n",
    "| IGNORADO       | Arquivo não encontrado no caminho indicado              |\n",
    "| ERRO           | Algum erro impediu a ação (detalhes no log)             |\n",
    "\n",
    "Esta etapa complementa o diagnóstico anterior ao aplicar efetivamente as correções necessárias na pasta `corpus_final`, preparando os arquivos para uso limpo e padronizado no projeto IAVS.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc55887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Execução concluída. Log salvo em: C:\\Users\\isisi\\Documents\\IAVS_PROJETO\\corpus_final\\log_execucao.csv\n"
     ]
    }
   ],
   "source": [
    "#executar_log_diagnostico remove duplicatas reais e renomeia arquivos com nome corrompido.Logo após o diagnóstico\n",
    "\n",
    "# === CONFIGURAÇÃO ===\n",
    "PASTA_BASE = Path(r\"C:\\Users\\isisi\\Documents\\IAVS_PROJETO\\corpus_final\")  # ajuste se necessário\n",
    "CAMINHO_LOG_DIAGNOSTICO = PASTA_BASE / \"log_diagnostico.csv\"# caminho do log de diagnóstico\n",
    "CAMINHO_LOG_EXECUCAO = PASTA_BASE / \"log_execucao.csv\" # caminho do log de execução\n",
    "\n",
    "# === LEITURA DO LOG ===\n",
    "with open(CAMINHO_LOG_DIAGNOSTICO, encoding='utf-8') as f: # abre o arquivo de log de diagnóstico\n",
    "    leitor = csv.DictReader(f)\n",
    "    acoes = list(leitor)\n",
    "\n",
    "# === EXECUÇÃO DAS AÇÕES ===\n",
    "log_execucao = [] # lista para armazenar o log de execução\n",
    "\n",
    "for linha in acoes: # itera sobre cada linha do log de diagnóstico\n",
    "    tipo = linha['Tipo']\n",
    "    caminho_str = linha['Arquivo']\n",
    "    caminho_arquivo = Path(caminho_str)\n",
    "\n",
    "    if not caminho_arquivo.exists(): # verifica se o arquivo existe\n",
    "        log_execucao.append([\"IGNORADO\", caminho_str, \"Arquivo não encontrado\"])\n",
    "        continue\n",
    "\n",
    "    if tipo == \"DUPLICADO\": # se o tipo for duplicado\n",
    "        try:\n",
    "            caminho_arquivo.unlink()\n",
    "            log_execucao.append([\"REMOVIDO\", caminho_str, \"Duplicata removida\"])\n",
    "            print(f\"🗑️ Removido: {caminho_str}\")\n",
    "        except Exception as e: # tenta remover o arquivo\n",
    "            log_execucao.append([\"ERRO\", caminho_str, f\"Erro ao remover: {e}\"])\n",
    "\n",
    "    elif tipo == \"NOME_CORROMPIDO\": # se o tipo for nome corrompido\n",
    "        sugestao = linha['Referência/Sugestão']\n",
    "        novo_caminho = caminho_arquivo.parent / sugestao\n",
    "        contador = 1\n",
    "        while novo_caminho.exists(): # verifica se o novo caminho já existe\n",
    "            novo_caminho = caminho_arquivo.parent / f\"{sugestao}_{contador}.txt\"\n",
    "            contador += 1\n",
    "        try:# tenta renomear o arquivo\n",
    "            caminho_arquivo.rename(novo_caminho)\n",
    "            log_execucao.append([\"RENOMEADO\", caminho_str, f\"Novo nome: {novo_caminho.name}\"])\n",
    "            print(f\"✏️ Renomeado: {caminho_arquivo.name} → {novo_caminho.name}\")\n",
    "        except Exception as e:\n",
    "            log_execucao.append([\"ERRO\", caminho_str, f\"Erro ao renomear: {e}\"])\n",
    "\n",
    "# === SALVA LOG FINAL ===\n",
    "with open(CAMINHO_LOG_EXECUCAO, mode='w', newline='', encoding='utf-8') as f: # abre o arquivo de log de execução\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Ação\", \"Arquivo\", \"Detalhes\"])\n",
    "    writer.writerows(log_execucao)\n",
    "\n",
    "print(f\"\\n✅ Execução concluída. Log salvo em: {CAMINHO_LOG_EXECUCAO}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef8f6e2",
   "metadata": {},
   "source": [
    "# Extração e Validação de Parágrafos Válidos\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"https://img.icons8.com/fluency/96/checked-checkbox.png\" width=\"80\" alt=\"Validação de Parágrafos\"/>\n",
    "</p>\n",
    "\n",
    "Este script percorre todos os arquivos `.txt` na pasta `corpus_final`, extrai os parágrafos brutos e filtra apenas os **parágrafos considerados válidos**, com o objetivo de construir uma base estruturada para análise e geração de perguntas.\n",
    "\n",
    "### Objetivos da etapa\n",
    "\n",
    "- Quebrar corretamente os textos em parágrafos sem ruídos de quebra de linha mal formatada.\n",
    "- Eliminar blocos irrelevantes como fichas técnicas, autoria, licenças e seções institucionais.\n",
    "- Salvar os parágrafos limpos em formatos compatíveis com análise tabular (Excel e CSV).\n",
    "\n",
    "### Critérios de validade de parágrafos\n",
    "\n",
    "Um parágrafo é considerado **inválido** se:\n",
    "- Contém menos de 5 palavras;\n",
    "- É composto apenas por números ou bullets (ex: \"•\", \"*\");\n",
    "- Contém **dois ou mais termos institucionais** como \"ministério\", \"licença\", \"tiragem\", \"autores\", etc.\n",
    "\n",
    "### Funcionalidades do script\n",
    "\n",
    "- `quebrar_paragrafos_brutos()`: divide o texto com base em pontuação e letras maiúsculas, simulando a quebra natural de parágrafos.\n",
    "- `paragrafo_valido()`: aplica os filtros para garantir que apenas conteúdo relevante permaneça.\n",
    "- `limpar_caracteres_invalidos()`: remove símbolos que podem causar erro na exportação para Excel.\n",
    "\n",
    "### Arquivos gerados\n",
    "\n",
    "- **`paragrafos_validos_corpus.xlsx`**: contém os parágrafos válidos organizados por arquivo e número do parágrafo.\n",
    "- **`paragrafos_validos_corpus.csv`**: versão em CSV dos mesmos dados.\n",
    "- **`diagnostico_paragrafos_por_arquivo.xlsx`**: relatório com o total de parágrafos lidos, quantos foram mantidos e quantos foram descartados por arquivo.\n",
    "\n",
    "### Exemplo de estrutura da saída:\n",
    "\n",
    "| Arquivo               | Número do parágrafo | Parágrafo                            |\n",
    "|-----------------------|---------------------|--------------------------------------|\n",
    "| dengue_guia2023.txt   | 1                   | A dengue é uma doença viral...       |\n",
    "| dengue_guia2023.txt   | 2                   | A transmissão ocorre pela picada... |\n",
    "\n",
    "| Arquivo               | Total lidos | Válidos mantidos | Descartados |\n",
    "|-----------------------|-------------|------------------|-------------|\n",
    "| dengue_guia2023.txt   | 150         | 87               | 63          |\n",
    "\n",
    "Essa etapa garante que o corpus textual a ser utilizado em análises futuras esteja **limpo, bem segmentado e sem conteúdos irrelevantes**, servindo como base sólida para tarefas como geração automática de perguntas e avaliação de IA.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cadef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📄 Parágrafos válidos salvos em C:\\Users\\isisi\\Documents\\IAVS_PROJETO\\corpus_final\\paragrafos_validos_corpus.xlsx\n",
      "✅ 20106 parágrafos válidos salvos em C:\\Users\\isisi\\Documents\\IAVS_PROJETO\\corpus_final\\paragrafos_validos_corpus.xlsx\n",
      "🩺 Diagnóstico salvo em C:\\Users\\isisi\\Documents\\IAVS_PROJETO\\corpus_final\\diagnostico_paragrafos_por_arquivo.xlsx\n"
     ]
    }
   ],
   "source": [
    "# === CONFIGURAÇÃO ===\n",
    "PASTA_CORPUS = Path(r\"C:\\Users\\isisi\\Documents\\IAVS_PROJETO\\corpus_final\")\n",
    "ARQUIVO_SAIDA = PASTA_CORPUS / \"paragrafos_validos_corpus.xlsx\"\n",
    "ARQUIVO_DIAGNOSTICO = PASTA_CORPUS / \"diagnostico_paragrafos_por_arquivo.xlsx\"\n",
    "\n",
    "# === FUNÇÕES ===\n",
    "def paragrafo_valido(paragrafo):\n",
    "    paragrafo = paragrafo.strip()\n",
    "    if len(paragrafo.split()) < 5:\n",
    "        return False\n",
    "    if re.match(r'^(\\d+|[•*])$', paragrafo):  # apenas número ou bullet\n",
    "        return False\n",
    "    if sum(1 for palavra in [\"ministério\", \"licença\", \"tiragem\", \"elaboração\", \"cep\", \"autores\", \"versão\", \"editoração\"] if palavra in paragrafo.lower()) >= 2:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def limpar_caracteres_invalidos(texto):\n",
    "    return ILLEGAL_CHARACTERS_RE.sub(\"\", texto)\n",
    "\n",
    "def quebrar_paragrafos_brutos(texto):\n",
    "    # Substitui quebras de linha simples por espaço (evita quebra errada)\n",
    "    texto = texto.replace(\"\\n\", \" \")\n",
    "    # Insere quebra forçada após ponto final seguido de espaço e letra maiúscula\n",
    "    texto = re.sub(r'\\. (?=[A-ZÁÉÍÓÚÇ])', '.\\n\\n', texto)\n",
    "    # Divide onde houver \\n\\n forçado\n",
    "    return [p.strip() for p in texto.split('\\n\\n') if p.strip()]\n",
    "\n",
    "# === PROCESSAMENTO ===\n",
    "dados = []\n",
    "diagnostico = []\n",
    "\n",
    "for caminho_txt in PASTA_CORPUS.rglob(\"*.txt\"):\n",
    "    try:\n",
    "        with open(caminho_txt, \"r\", encoding=\"utf-8\") as f:\n",
    "            conteudo = f.read()\n",
    "\n",
    "        # Divide usando segmentação forçada\n",
    "        paragrafos_raw = quebrar_paragrafos_brutos(conteudo)\n",
    "        paragrafos_validos = [p for p in paragrafos_raw if paragrafo_valido(p)]\n",
    "\n",
    "        for i, paragrafo in enumerate(paragrafos_validos, 1):\n",
    "            dados.append({\n",
    "                \"Arquivo\": caminho_txt.name,\n",
    "                \"Parágrafo\": limpar_caracteres_invalidos(paragrafo),\n",
    "                \"Número do parágrafo\": i\n",
    "            })\n",
    "\n",
    "        diagnostico.append({\n",
    "            \"Arquivo\": caminho_txt.name,\n",
    "            \"Total lidos\": len(paragrafos_raw),\n",
    "            \"Válidos mantidos\": len(paragrafos_validos),\n",
    "            \"Descartados\": len(paragrafos_raw) - len(paragrafos_validos)\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Erro ao processar {caminho_txt.name}: {e}\")\n",
    "\n",
    "# === EXPORTAÇÃO ===\n",
    "df_paragrafos = pd.DataFrame(dados)\n",
    "df_paragrafos.to_excel(ARQUIVO_SAIDA, index=False)\n",
    "\n",
    "df_diag = pd.DataFrame(diagnostico)\n",
    "df_diag.to_excel(ARQUIVO_DIAGNOSTICO, index=False)\n",
    "\n",
    "ARQUIVO_CSV = PASTA_CORPUS / \"paragrafos_validos_corpus.csv\"\n",
    "df_paragrafos.to_csv(ARQUIVO_CSV, index=False, encoding=\"utf-8\")\n",
    "print(f\"📄 Parágrafos válidos salvos em {ARQUIVO_SAIDA}\")\n",
    "print(f\"✅ {len(df_paragrafos)} parágrafos válidos salvos em {ARQUIVO_SAIDA}\")\n",
    "print(f\"🩺 Diagnóstico salvo em {ARQUIVO_DIAGNOSTICO}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bbf239",
   "metadata": {},
   "source": [
    "# Consolidação e Resumo dos Arquivos Únicos (.CSV)\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"https://img.icons8.com/ios-filled/100/document--v1.png\" width=\"80\" alt=\"Arquivos Únicos\"/>\n",
    "</p>\n",
    "\n",
    "Esta etapa realiza a consolidação e análise dos arquivos presentes em `corpus_final`, utilizando o arquivo de entrada `resumo_corpus_final.csv`, que contém metadados como tamanho, número de linhas e caracteres por arquivo.\n",
    "\n",
    "### Objetivos da etapa\n",
    "\n",
    "- **Identificar e manter apenas arquivos únicos**, mesmo quando há nomes duplicados ou corrompidos (ex: `dengue_txttxt.txt`).\n",
    "- **Agrupar e sumarizar por subpasta**, fornecendo uma visão geral da distribuição de documentos por tema ou doença.\n",
    "\n",
    "### O que o script faz\n",
    "\n",
    "- **Importa o CSV de entrada** com os metadados de todos os arquivos `.txt`.\n",
    "- **Normaliza os nomes de arquivo**, criando a coluna `Arquivo_base` para eliminar duplicações.\n",
    "- **Filtra os arquivos únicos** por subpasta e nome base.\n",
    "- **Agrupa por subpasta**, calculando:\n",
    "  - Total de arquivos únicos\n",
    "  - Tamanho total (bytes)\n",
    "  - Média de linhas por arquivo\n",
    "  - Média de caracteres por arquivo\n",
    "\n",
    "### Arquivos gerados\n",
    "\n",
    "- `resumo_arquivos_unicos.csv`: lista dos arquivos únicos por subpasta.\n",
    "- `resumo_por_subpasta.csv`: tabela de resumo consolidado por subpasta.\n",
    "\n",
    "| Subpasta    | Total de arquivos únicos | Tamanho total (bytes) | Média de linhas | Média de caracteres |\n",
    "|-------------|---------------------------|------------------------|------------------|----------------------|\n",
    "| dengue      | 8                         | 153000                 | 117              | 5300                 |\n",
    "| tuberculose | 6                         | 94000                  | 98               | 4100                 |\n",
    "\n",
    "### Observações\n",
    "\n",
    "- A deduplicação considera tanto o nome quanto a subpasta.\n",
    "- Os arquivos finais estão em formato `.csv`, permitindo fácil importação por sistemas externos, planilhas e painéis.\n",
    "- Esta etapa depende da consistência dos nomes gerados nas etapas anteriores (correção e diagnóstico).\n",
    "\n",
    "O resultado é um panorama claro e confiável da composição do corpus final, essencial para validações e planejamentos posteriores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3879a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 39\n",
      "✅ Arquivo único salvo em: C:\\Users\\isisi\\Documents\\IAVS_PROJETO\\corpus_final\\resumo_arquivos_unicos.xlsx\n",
      "📊 Resumo por subpasta salvo em: C:\\Users\\isisi\\Documents\\IAVS_PROJETO\\corpus_final\\resumo_por_subpasta.xlsx\n"
     ]
    }
   ],
   "source": [
    "# === CONFIGURAÇÃO ===\n",
    "PASTA_BASE = Path(r\"C:\\Users\\isisi\\Documents\\IAVS_PROJETO\\corpus_final\") #identifica a pasta base onde os arquivos estão localizados\n",
    "ARQUIVO_ENTRADA = PASTA_BASE / \"resumo_corpus_final.csv\"# caminho do arquivo CSV de entrada com o resumo dos arquivos\n",
    "ARQUIVO_UNICOS = PASTA_BASE / \"resumo_arquivos_unicos.csv\"# caminho do arquivo de saída com os arquivos únicos\n",
    "ARQUIVO_AGREGADO = PASTA_BASE / \"resumo_por_subpasta.csv\" # caminho do arquivo de saída com o resumo por subpasta\n",
    "\n",
    "# === LEITURA DO ARQUIVO DE RESUMO ===\n",
    "df = pd.read_csv(ARQUIVO_ENTRADA)\n",
    "\n",
    "# === REMOVER DUPLICATAS POR NOME BASE DO ARQUIVO ===\n",
    "# Extrai o nome base (sem partes repetidas)\n",
    "df[\"Arquivo_base\"] = df[\"Arquivo\"].str.extract(r\"^(.+?)(?:_?txt)+(?:\\.txt)?$\", expand=False).str.strip()#aqui, usamos uma expressão regular para extrair o nome base do arquivo, removendo partes repetidas como \"_txt\" ou \".txt\". Isso ajuda a identificar arquivos com nomes semelhantes.\n",
    "df_unicos = df.drop_duplicates(subset=[\"Subpasta\", \"Arquivo_base\"])#já aqui, removemos duplicatas com base na subpasta e no nome base do arquivo. Isso garante que apenas um arquivo por subpasta e nome base seja mantido.\n",
    "\n",
    "# === AGRUPAMENTO POR SUBPASTA ===\n",
    "# Agrupa os dados por subpasta e calcula as estatísticas desejadas\n",
    "df_grupo = df_unicos.groupby(\"Subpasta\").agg({\n",
    "    \"Arquivo\": \"count\",\n",
    "    \"Tamanho (bytes)\": \"sum\",\n",
    "    \"Nº de linhas\": \"mean\",\n",
    "    \"Nº de caracteres\": \"mean\"\n",
    "}).reset_index()\n",
    "\n",
    "df_grupo = df_grupo.rename(columns={\n",
    "    \"Arquivo\": \"Total de arquivos únicos\",\n",
    "    \"Tamanho (bytes)\": \"Tamanho total (bytes)\",\n",
    "    \"Nº de linhas\": \"Média de linhas\",\n",
    "    \"Nº de caracteres\": \"Média de caracteres\"\n",
    "})\n",
    "\n",
    "# === EXPORTAÇÃO ===\n",
    "# (Removido: df_unicos.to_excel(ARQUIVO_UNICOS, index=False))\n",
    "df_unicos.to_csv(ARQUIVO_UNICOS, index=False, encoding=\"utf-8\")\n",
    "df_grupo.to_csv(ARQUIVO_AGREGADO, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"✅ Arquivo único salvo em: {ARQUIVO_UNICOS}\")\n",
    "print(f\"📊 Resumo por subpasta salvo em: {ARQUIVO_AGREGADO}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee86fa1",
   "metadata": {},
   "source": [
    "# Geração do Corpus Unificado (`corpus_unificado.txt`)\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://img.icons8.com/ios-filled/100/idea.png\" width=\"80\" alt=\"Lâmpada - Ideia\"/>\n",
    "</p>\n",
    "\n",
    "Após as etapas de extração, limpeza, deduplicação e validação dos arquivos `.txt`, esta etapa consolida todo o conteúdo textual da pasta `corpus_final` em um único arquivo chamado `corpus_unificado.txt`.\n",
    "\n",
    "### Objetivos da etapa\n",
    "\n",
    "- **Agrupar todos os arquivos limpos** em um único arquivo de texto.\n",
    "- **Preservar a estrutura organizacional** dos documentos, mantendo a identificação por subpasta (tema/doença) e nome do arquivo.\n",
    "- **Facilitar a inspeção visual**, revisão manual ou busca textual do corpus consolidado.\n",
    "\n",
    "### O que o script faz\n",
    "\n",
    "- Percorre recursivamente todos os arquivos `.txt` da pasta `corpus_final`.\n",
    "- Ignora arquivos muito curtos (menos de 50 caracteres), que provavelmente são inválidos ou vazios.\n",
    "- Para cada arquivo válido:\n",
    "  - Lê o conteúdo e remove espaços em branco no início e fim.\n",
    "  - Identifica o nome do arquivo e a subpasta a que pertence.\n",
    "  - Adiciona ao `corpus_unificado.txt` com um cabeçalho no formato:\n",
    "\n",
    "    ### Arquivo: nome_do_arquivo.txt | Subpasta: nome_da_subpasta\n",
    "    ```\n",
    "\n",
    "- Salva o resultado em `corpus_final/corpus_unificado.txt`.\n",
    "\n",
    "### Finalidade do arquivo `corpus_unificado.txt`\n",
    "\n",
    "- Permite uma **validação rápida do conteúdo final** do corpus.\n",
    "- Pode ser aberto em qualquer editor de texto para:\n",
    "  - Revisão por especialistas;\n",
    "  - Busca de termos relevantes;\n",
    "  - Conferência de organização e integridade textual;\n",
    "  - Geração de índices ou relatórios.\n",
    "\n",
    "### Exemplo de estrutura no arquivo final\n",
    "\n",
    "Arquivo: dengue_transmissao.txt | Subpasta: dengue\n",
    "A dengue é uma doença viral transmitida principalmente por mosquitos do gênero Aedes...\n",
    "\n",
    "Arquivo: hanseniase_tratamento.txt | Subpasta: hanseniase\n",
    "O tratamento da hanseníase é ofertado gratuitamente pelo SUS e baseado na poliquimioterapia...\n",
    "\n",
    "\n",
    "### Observações\n",
    "\n",
    "- O script pode ser executado sempre que houver atualização nos textos limpos.\n",
    "- Ele não altera os arquivos originais, apenas lê e concatena.\n",
    "- Útil para revisar o corpus como um todo sem precisar abrir os arquivos individualmente.\n",
    "\n",
    "Esta etapa finaliza a construção do corpus textual unificado, sendo um ponto de partida para processos de análise automática, revisão qualitativa ou auditorias de conteúdo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82afd0f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Corpus unificado salvo em: C:\\Users\\isisi\\Documents\\IAVS_PROJETO\\corpus_final\\corpus_unificado.txt\n"
     ]
    }
   ],
   "source": [
    "# === CONFIGURAÇÕES ===\n",
    "PASTA_CORPUS = Path(r\"C:\\Users\\isisi\\Documents\\IAVS_PROJETO\\corpus_final\")# identifica a pasta base onde os arquivos estão localizados\n",
    "ARQUIVO_SAIDA = PASTA_CORPUS / \"corpus_unificado.txt\"# caminho do arquivo de saída onde o corpus unificado será salvo\n",
    "\n",
    "# === UNIFICAR CONTEÚDO ===\n",
    "with open(ARQUIVO_SAIDA, \"w\", encoding=\"utf-8\") as arquivo_saida: # abre o arquivo de saída para escrita\n",
    "    for caminho_txt in PASTA_CORPUS.rglob(\"*.txt\"):\n",
    "        try:\n",
    "            with open(caminho_txt, \"r\", encoding=\"utf-8\") as f:\n",
    "                conteudo = f.read().strip()\n",
    "\n",
    "            if len(conteudo) < 50:\n",
    "                continue  # ignora arquivos muito curtos\n",
    "\n",
    "            nome_arquivo = caminho_txt.name\n",
    "            subpasta = caminho_txt.parent.name\n",
    "\n",
    "            bloco = f\"\\n\\n### Arquivo: {nome_arquivo} | Subpasta: {subpasta}\\n\\n{conteudo}\\n\\n\"\n",
    "            arquivo_saida.write(bloco)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao ler {caminho_txt.name}: {e}\") # ignora erros de leitura\n",
    "\n",
    "print(f\"✅ Corpus unificado salvo em: {ARQUIVO_SAIDA}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff10d7a",
   "metadata": {},
   "source": [
    "# <img src=\"https://img.icons8.com/ios-filled/100/document--v1.png\" width=\"40\" style=\"vertical-align:middle; margin-right:8px;\"/> Filtragem Semântica Rígida dos Parágrafos\n",
    "\n",
    "\n",
    "Esta etapa aplica uma filtragem semântica rigorosa ao conjunto de parágrafos válidos previamente extraídos, com o objetivo de manter **apenas os trechos realmente relevantes para análise textual ou treinamento de modelos de IA**.\n",
    "\n",
    "### Objetivos da etapa\n",
    "\n",
    "- **Descartar parágrafos institucionais ou técnicos irrelevantes**, como autoria, endereço, ficha catalográfica e licenciamento.\n",
    "- **Evitar parágrafos com conteúdo residual**, como listas de anexo, sumário ou campos vazios.\n",
    "- **Aplicar um filtro mais protetivo para segurança e qualidade** do corpus final.\n",
    "\n",
    "### Funcionalidades do script\n",
    "\n",
    "- **Leitura do arquivo `paragrafos_validos_corpus.csv`**, que contém todos os parágrafos classificados como válidos em etapas anteriores.\n",
    "- **Aplicação de filtros baseados em regras linguísticas e estruturais**, incluindo:\n",
    "  - Tamanho máximo de 3000 caracteres.\n",
    "  - Exclusão por presença de padrões indesejáveis (ex: “nome do autor”, “www.”, “ministério da saúde”, “tiragem”, “índice”, etc.).\n",
    "  - Exclusão de parágrafos com menos de 6 palavras.\n",
    "\n",
    "- **Separação dos dados** em dois conjuntos:\n",
    "  - `paragrafos_filtrados_corpus.csv`: apenas os parágrafos considerados relevantes.\n",
    "  - `paragrafos_descartados.csv`: parágrafos eliminados por não atenderem aos critérios.\n",
    "\n",
    "- **Geração de um relatório quantitativo (`relatorio_filtragem_paragrafos.csv`)** com estatísticas resumidas do processo de filtragem.\n",
    "\n",
    "### Exemplo de padrões que são filtrados\n",
    "\n",
    "| Tipo de conteúdo excluído           | Exemplo de padrão identificado                     |\n",
    "|-------------------------------------|----------------------------------------------------|\n",
    "| Identificação institucional         | `Ministério da Saúde`, `Coordenação-Geral`        |\n",
    "| Autoria ou ficha técnica            | `Autores:`, `Revisão:`, `Nome do autor:`          |\n",
    "| Informações logísticas e endereço  | `CEP:`, `Quadra`, `www.saude.gov.br`              |\n",
    "| Referências editoriais             | `Esta obra é licenciada...`, `Edição 2023`        |\n",
    "| Tópicos não informativos           | `Anexo`, `Índice`, `Apêndice`                     |\n",
    "\n",
    "### Arquivos gerados\n",
    "\n",
    "- **`paragrafos_filtrados_corpus.csv`** – Corpus final com apenas parágrafos relevantes.\n",
    "- **`paragrafos_descartados.csv`** – Arquivo com todos os parágrafos excluídos na filtragem.\n",
    "- **`relatorio_filtragem_paragrafos.csv`** – Relatório estatístico com o total de parágrafos, mantidos, excluídos e suas respectivas proporções.\n",
    "\n",
    "| Métrica         | Valor     |\n",
    "|-----------------|-----------|\n",
    "| Total original  | 12.000    |\n",
    "| Total mantido   | 7.400     |\n",
    "| Total excluído  | 4.600     |\n",
    "| % mantido       | 61.67%    |\n",
    "| % excluído      | 38.33%    |\n",
    "\n",
    "### Observações\n",
    "\n",
    "- O filtro é propositalmente conservador, priorizando precisão em detrimento de abrangência.\n",
    "- Os critérios podem ser ajustados posteriormente conforme o feedback da equipe técnica ou dos especialistas em conteúdo.\n",
    "\n",
    "Esta etapa garante que o corpus final esteja limpo, consistente e focado no conteúdo técnico-informativo, evitando ruídos que comprometam a qualidade da análise ou da aplicação de inteligência artificial.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e19876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CSV salvo com 17640 parágrafos: C:\\Users\\isisi\\Documents\\IAVS_PROJETO\\corpus_final\\paragrafos_filtrados_corpus.csv\n",
      "📊 Relatório salvo: C:\\Users\\isisi\\Documents\\IAVS_PROJETO\\corpus_final\\relatorio_filtragem_paragrafos.csv\n"
     ]
    }
   ],
   "source": [
    "# === CAMINHOS ===\n",
    "PASTA = r\"C:\\Users\\isisi\\Documents\\IAVS_PROJETO\\corpus_final\" # identifica a pasta base onde os arquivos estão localizados\n",
    "ARQUIVO_ENTRADA = os.path.join(PASTA, \"paragrafos_validos_corpus.csv\") # caminho do arquivo CSV de entrada com os parágrafos válidos\n",
    "ARQUIVO_SAIDA = os.path.join(PASTA, \"paragrafos_filtrados_corpus.csv\") # caminho do arquivo CSV de saída com os parágrafos filtrados\n",
    "ARQUIVO_RELATORIO = os.path.join(PASTA, \"relatorio_filtragem_paragrafos.csv\") # caminho do arquivo de relatório com estatísticas da filtragem\n",
    "\n",
    "# === FILTRO SEMÂNTICO COM PROTEÇÕES RÍGIDAS ===\n",
    "def paragrafo_relevante(texto):\n",
    "    texto_limpo = str(texto).strip()\n",
    "\n",
    "    if len(texto_limpo) > 3000:  # ainda mais seguro\n",
    "        return False\n",
    "    if ILLEGAL_CHARACTERS_RE.search(texto_limpo):\n",
    "        return False\n",
    "\n",
    "    texto_limpo = texto_limpo.lower()\n",
    "    padroes_excluir = [\n",
    "        r'nomes?[:\\-]',\n",
    "        r'e-mail|site:|www\\.',\n",
    "        r'cep:|quadra|srtv|endere',\n",
    "        r'coordenação-geral|departamento',\n",
    "        r'ministério da saúde',\n",
    "        r'esta obra.*licença',\n",
    "        r'(nome|cargo) do autor',\n",
    "        r'saúde pública brasileira.*acesso',\n",
    "        r'\\b(anexo|apêndice|índice)\\b',\n",
    "        r'(autores?|versão|tiragem|edição).*202[0-9]',\n",
    "        r'(sigla|significado) de ',\n",
    "        r'(organizador|responsável|revisão)',\n",
    "    ]\n",
    "    for padrao in padroes_excluir:\n",
    "        if re.search(padrao, texto_limpo):\n",
    "            return False\n",
    "    if len(texto_limpo.split()) < 6:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# === LEITURA ===\n",
    "df = pd.read_csv(ARQUIVO_ENTRADA)\n",
    "df[\"Relevante\"] = df[\"Parágrafo\"].astype(str).apply(paragrafo_relevante)\n",
    "\n",
    "# === SEPARAÇÃO ===\n",
    "df_filtrado = df[df[\"Relevante\"]].drop(columns=[\"Relevante\"])\n",
    "df_excluido = df[~df[\"Relevante\"]].drop(columns=[\"Relevante\"])\n",
    "\n",
    "# === SALVAR RESULTADOS EM .CSV ===\n",
    "df_filtrado.to_csv(ARQUIVO_SAIDA, index=False, encoding=\"utf-8\")\n",
    "df_excluido.to_csv(PASTA + \"/paragrafos_descartados.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "# === RELATÓRIO RESUMIDO ===\n",
    "resumo = pd.DataFrame([{\n",
    "    \"Total original\": len(df),\n",
    "    \"Total mantido\": len(df_filtrado),\n",
    "    \"Total excluído\": len(df_excluido),\n",
    "    \"% mantido\": round(100 * len(df_filtrado) / len(df), 2),\n",
    "    \"% excluído\": round(100 * len(df_excluido) / len(df), 2),\n",
    "}])\n",
    "resumo.to_csv(ARQUIVO_RELATORIO, index=False)\n",
    "\n",
    "print(f\"✅ CSV salvo com {len(df_filtrado)} parágrafos: {ARQUIVO_SAIDA}\")\n",
    "print(f\"📊 Relatório salvo: {ARQUIVO_RELATORIO}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757523d5",
   "metadata": {},
   "source": [
    "# <img src=\"https://img.icons8.com/ios-filled/100/ask-question.png\" width=\"40\" style=\"vertical-align:middle; margin-right:8px;\"/> Geração de Perguntas com Modelo T5\n",
    "\n",
    "Esta etapa aplica **geração automática de perguntas** a partir dos parágrafos previamente filtrados, utilizando o modelo de linguagem `valhalla/t5-base-qg-hl`. O objetivo é criar uma base de questões que poderá ser usada para treinamento, avaliação de IA ou análise de compreensão textual.\n",
    "\n",
    "O **T5 (Text-to-Text Transfer Transformer)** é um modelo de linguagem desenvolvido pelo Google que converte todas as tarefas de processamento de linguagem natural em um formato de entrada e saída de texto para texto. Isso significa que, para tarefas como tradução, resumo, resposta a perguntas ou geração de perguntas, o T5 recebe uma instrução textual e retorna uma resposta também em texto. Sua arquitetura baseada em transformers permite grande flexibilidade e desempenho em múltiplas tarefas, sendo amplamente utilizado para aplicações avançadas de NLP.\n",
    "\n",
    "### Objetivos da etapa\n",
    "\n",
    "- Gerar uma pergunta para cada parágrafo considerado relevante no corpus.\n",
    "- Dividir o processo em blocos para **evitar sobrecarga de memória e facilitar retomadas**.\n",
    "- Salvar os resultados em arquivos separados por lote.\n",
    "\n",
    "### Funcionalidades do script\n",
    "\n",
    "- **Carregamento do modelo T5** e tokenizer (`valhalla/t5-base-qg-hl`) com suporte a GPU (caso disponível).\n",
    "- **Leitura do arquivo `paragrafos_filtrados_corpus.csv`**, contendo os textos-base para geração.\n",
    "- **Divisão do corpus em blocos de 1000 parágrafos**, controlando o tamanho dos lotes para evitar falhas em máquinas com menos memória.\n",
    "- **Geração de perguntas** com base na instrução `generate question: <texto>`, em que o modelo entende que deve formular uma pergunta sobre o conteúdo apresentado.\n",
    "- **Exportação dos blocos** em arquivos CSV nomeados sequencialmente, por exemplo: `perguntas_001.csv`, `perguntas_002.csv`, etc.\n",
    "- **Verificação automática de blocos já processados**, evitando retrabalho em execuções futuras.\n",
    "\n",
    "### Estrutura do arquivo gerado por bloco\n",
    "\n",
    "| Parágrafo                                                     | Pergunta                                      |\n",
    "|---------------------------------------------------------------|-----------------------------------------------|\n",
    "| A dengue é uma doença viral transmitida por mosquitos...      | Qual é o modo de transmissão da dengue?       |\n",
    "| O tratamento da tuberculose dura no mínimo seis meses...      | Quanto tempo dura o tratamento da tuberculose?|\n",
    "\n",
    "### Organização dos resultados\n",
    "\n",
    "- As perguntas são salvas em uma subpasta com data do dia de execução, ex:  \n",
    "  `corpus_final/perguntas_geradas_20250803/`\n",
    "- Cada arquivo contém até 1000 pares de parágrafo/pergunta.\n",
    "- A pasta de saída é criada automaticamente, se ainda não existir.\n",
    "\n",
    "### Observações\n",
    "\n",
    "- A geração é feita com **máximo de 64 tokens** por pergunta para garantir objetividade.\n",
    "- O script pode ser executado novamente sem sobrescrever resultados já salvos, tornando-o seguro para execuções parciais.\n",
    "- O modelo `valhalla/t5-base-qg-hl` foi escolhido por seu bom desempenho em tarefas de question generation em português e inglês, mas pode ser substituído se necessário.\n",
    "\n",
    "Essa etapa transforma o corpus textual em uma base estruturada de perguntas, pronta para ser usada em avaliações manuais, benchmarking de IA, ou sistemas de apoio ao ensino e treinamento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5101c328",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade torch --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12ae1e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏭️ Bloco 1 já processado: C:\\Users\\isisi\\Documents\\IAVS_PROJETO\\corpus_final\\perguntas_geradas_20250804\\perguntas_001.csv\n",
      "⏭️ Bloco 2 já processado: C:\\Users\\isisi\\Documents\\IAVS_PROJETO\\corpus_final\\perguntas_geradas_20250804\\perguntas_002.csv\n",
      "⏭️ Bloco 3 já processado: C:\\Users\\isisi\\Documents\\IAVS_PROJETO\\corpus_final\\perguntas_geradas_20250804\\perguntas_003.csv\n",
      "⏭️ Bloco 4 já processado: C:\\Users\\isisi\\Documents\\IAVS_PROJETO\\corpus_final\\perguntas_geradas_20250804\\perguntas_004.csv\n",
      "⏭️ Bloco 5 já processado: C:\\Users\\isisi\\Documents\\IAVS_PROJETO\\corpus_final\\perguntas_geradas_20250804\\perguntas_005.csv\n",
      "⏭️ Bloco 6 já processado: C:\\Users\\isisi\\Documents\\IAVS_PROJETO\\corpus_final\\perguntas_geradas_20250804\\perguntas_006.csv\n",
      "⏭️ Bloco 7 já processado: C:\\Users\\isisi\\Documents\\IAVS_PROJETO\\corpus_final\\perguntas_geradas_20250804\\perguntas_007.csv\n",
      "⏭️ Bloco 8 já processado: C:\\Users\\isisi\\Documents\\IAVS_PROJETO\\corpus_final\\perguntas_geradas_20250804\\perguntas_008.csv\n",
      "⏭️ Bloco 9 já processado: C:\\Users\\isisi\\Documents\\IAVS_PROJETO\\corpus_final\\perguntas_geradas_20250804\\perguntas_009.csv\n",
      "⏭️ Bloco 10 já processado: C:\\Users\\isisi\\Documents\\IAVS_PROJETO\\corpus_final\\perguntas_geradas_20250804\\perguntas_010.csv\n",
      "🔄 Gerando perguntas para bloco 11 (10000–11000)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bloco 11:   4%|▎         | 36/1000 [00:13<04:33,  3.52it/s]"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# === CONFIGURAÇÕES ===\n",
    "PASTA = r\"C:\\Users\\isisi\\Documents\\IAVS_PROJETO\\corpus_final\" # Identifica a pasta base onde os arquivos estão localizados\n",
    "ARQUIVO_ENTRADA = os.path.join(PASTA, \"paragrafos_filtrados_corpus.csv\") # Caminho do arquivo CSV de entrada com os parágrafos filtrados\n",
    "DATA = datetime.today().strftime('%Y%m%d') # Obtém a data atual no formato YYYYMMDD\n",
    "PASTA_SAIDA = os.path.join(PASTA, f\"perguntas_geradas_{DATA}\")# Caminho da pasta de saída onde as perguntas geradas serão salvas\n",
    "os.makedirs(PASTA_SAIDA, exist_ok=True)# Cria a pasta de saída se não existir\n",
    "\n",
    "BLOCO_TAMANHO = 1000 #define o tamanho do bloco de processamento, ou seja, quantas linhas serão processadas por vez\n",
    "\n",
    "# === MODELO DE PERGUNTAS ===\n",
    "modelo = \"mrm8488/t5-base-finetuned-question-generation-ap\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(modelo) \n",
    "model = T5ForConditionalGeneration.from_pretrained(modelo) \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device) \n",
    "\n",
    "# === FUNÇÃO DE GERAÇÃO ===\n",
    "def gerar_pergunta(texto):\n",
    "    entrada = f\"generate question: {texto}\"\n",
    "    inputs = tokenizer(entrada, return_tensors=\"pt\", truncation=True).to(device)\n",
    "    outputs = model.generate(inputs[\"input_ids\"], max_length=64)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# === CARREGAR DADOS ===\n",
    "df = pd.read_csv(ARQUIVO_ENTRADA).reset_index(drop=True)\n",
    "total_linhas = len(df)\n",
    "num_blocos = (total_linhas + BLOCO_TAMANHO - 1) // BLOCO_TAMANHO  # Arredonda para cima\n",
    "\n",
    "# === PROCESSAMENTO POR BLOCO ===\n",
    "# === PROCESSAMENTO POR BLOCO ===\n",
    "for i in range(num_blocos):\n",
    "    inicio_tempo = time.time()\n",
    "    inicio = i * BLOCO_TAMANHO\n",
    "    fim = min((i + 1) * BLOCO_TAMANHO, total_linhas)\n",
    "    bloco_df = df.iloc[inicio:fim].copy()\n",
    "    saida_parcial = os.path.join(PASTA_SAIDA, f\"perguntas_{i+1:03}.csv\")\n",
    "\n",
    "    # Libera memória da GPU ANTES\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Pular se já existir\n",
    "    if os.path.exists(saida_parcial):\n",
    "        print(f\"⏭️ Bloco {i+1} já processado: {saida_parcial}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"🔄 Gerando perguntas para bloco {i+1} ({inicio}–{fim})...\")\n",
    "\n",
    "    perguntas = []\n",
    "    for paragrafo in tqdm(bloco_df[\"Parágrafo\"].astype(str), desc=f\"Bloco {i+1}\"):\n",
    "        try:\n",
    "            pergunta = gerar_pergunta(paragrafo)\n",
    "        except Exception as e:\n",
    "            pergunta = f\"[Erro: {str(e)}]\"\n",
    "        perguntas.append(pergunta)\n",
    "\n",
    "        # Libera GPU entre cada geração, se necessário\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    fim_tempo = time.time()\n",
    "    print(f\"⏱️ Tempo gasto no bloco {i+1}: {fim_tempo - inicio_tempo:.2f} segundos\")\n",
    "\n",
    "    bloco_df[\"Pergunta\"] = perguntas\n",
    "    bloco_df.to_csv(saida_parcial, index=False, encoding=\"utf-8\")\n",
    "    print(f\"✅ Perguntas salvas em: {saida_parcial}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-toolbox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
