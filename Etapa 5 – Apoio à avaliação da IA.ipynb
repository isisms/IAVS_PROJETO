{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07868495",
   "metadata": {},
   "source": [
    "# Etapa 5 ‚Äì Apoio √† Avalia√ß√£o da IA\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"https://img.icons8.com/ios-filled/100/artificial-intelligence.png\" width=\"80\" alt=\"√çcone IA\"/>\n",
    "</p>\n",
    "Subetapa: Filtragem dos Par√°grafos para Gera√ß√£o de Perguntas\n",
    "\n",
    "<img src=\"https://img.icons8.com/ios-filled/50/000000/right--v1.png\" width=\"24\" style=\"vertical-align:middle; margin-right:8px;\"/> Objetivo da Subetapa\n",
    "Durante a prepara√ß√£o do corpus para a gera√ß√£o autom√°tica de perguntas, observou-se que alguns arquivos .txt, mesmo ap√≥s as etapas anteriores de limpeza, ainda continham trechos irrelevantes. Esses trechos, quando enviados ao modelo de NLP, causavam lentid√£o no processamento e geravam perguntas mal formuladas, com baixa utilidade para fins de valida√ß√£o.\n",
    "\n",
    "Para resolver esse gargalo, foi desenvolvido um script de filtragem sem√¢ntica automatizada. Ele identifica e remove par√°grafos com conte√∫do n√£o t√©cnico ou administrativo, como nomes de autores, contatos institucionais, fichas catalogr√°ficas, endere√ßos e notas editoriais.\n",
    "\n",
    "**O que o Script Faz**\n",
    "O script executa as seguintes tarefas:\n",
    "\n",
    "- L√™ o arquivo `paragrafos_validos_corpus.xlsx`, que cont√©m todos os par√°grafos extra√≠dos dos arquivos .txt considerados v√°lidos estruturalmente.\n",
    "- Aplica uma fun√ß√£o de filtragem sem√¢ntica (`paragrafo_relevante`):\n",
    "    - Remove par√°grafos curtos (com menos de 6 palavras).\n",
    "    - Exclui par√°grafos que contenham padr√µes de texto como:\n",
    "        - Nomes de autores e respons√°veis t√©cnicos;\n",
    "        - Endere√ßos e e-mails;\n",
    "        - Informa√ß√µes de publica√ß√£o (tiragem, edi√ß√£o, vers√£o);\n",
    "        - Trechos gen√©ricos como ‚Äúanexo‚Äù, ‚Äú√≠ndice‚Äù, ‚Äúap√™ndice‚Äù;\n",
    "        - Informa√ß√µes institucionais repetitivas (ex: ‚ÄúMinist√©rio da Sa√∫de‚Äù).\n",
    "\n",
    "**Cria dois novos arquivos:** <img src=\"https://img.icons8.com/ios-filled/50/000000/document--v1.png\" width=\"24\" style=\"vertical-align:middle; margin-left:8px;\"/>\n",
    "\n",
    "*paragrafos_filtrados_corpus.xlsx: apenas com os par√°grafos considerados relevantes.\n",
    "\n",
    "relatorio_filtragem_paragrafos.xlsx: com estat√≠sticas da filtragem (totais e percentuais) e amostras dos par√°grafos exclu√≠dos e mantidos.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96aa85f",
   "metadata": {},
   "source": [
    "# Diagn√≥stico Visual de Duplicados e Nomes Corrompidos\n",
    "\n",
    "Este script realiza uma varredura completa nos arquivos `.txt` localizados na pasta `corpus_final` (ou outra indicada pelo usu√°rio), com o objetivo de identificar:\n",
    "\n",
    "- **Arquivos duplicados por conte√∫do**: a partir do c√°lculo de hash parcial dos arquivos (1 MB inicial), o script detecta textos que possuem exatamente o mesmo conte√∫do, ainda que estejam com nomes diferentes.\n",
    "- **Arquivos com nomes suspeitos ou corrompidos**: s√£o detectadas repeti√ß√µes an√¥malas em nomes como `arquivo_txt_txt_txt_2023.txt`, sugerindo um nome mais limpo.\n",
    "\n",
    "### Funcionalidades principais\n",
    "\n",
    "- **Interface gr√°fica interativa**: o usu√°rio escolhe a pasta desejada via bot√£o e visualiza os resultados diretamente na tela.\n",
    "- **Log autom√°tico**: os resultados do diagn√≥stico s√£o salvos em um arquivo `log_diagnostico.csv`, contendo os arquivos suspeitos de duplica√ß√£o ou com nomes inconsistentes.\n",
    "- **Execu√ß√£o segura**: o script apenas **l√™** os arquivos ‚Äî nenhuma altera√ß√£o √© feita nos nomes ou conte√∫dos durante o diagn√≥stico.\n",
    "\n",
    "### Principais fun√ß√µes do script\n",
    "\n",
    "- `calcular_hash()`: gera uma hash para os primeiros 1MB do conte√∫do do arquivo, permitindo identificar duplica√ß√µes de forma eficiente.\n",
    "- `normalizar_nome()`: detecta e prop√µe corre√ß√µes para nomes de arquivos com repeti√ß√µes incorretas de padr√µes.\n",
    "- `diagnosticar_pasta()`: realiza a varredura dos arquivos e imprime os resultados na interface, al√©m de registrar um log.\n",
    "- `iniciar_interface()`: cria a interface gr√°fica com campos de entrada, bot√µes e √°rea de visualiza√ß√£o dos resultados.\n",
    "\n",
    "Essa etapa √© essencial para garantir que a base de dados textual final n√£o contenha arquivos redundantes nem nomes mal formatados, facilitando a organiza√ß√£o e os pr√≥ximos passos do projeto.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "694747f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\isisi\\anaconda3\\envs\\anaconda-toolbox\\lib\\site-packages (4.54.1)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\isisi\\anaconda3\\envs\\anaconda-toolbox\\lib\\site-packages (0.2.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\isisi\\anaconda3\\envs\\anaconda-toolbox\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\isisi\\anaconda3\\envs\\anaconda-toolbox\\lib\\site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\isisi\\anaconda3\\envs\\anaconda-toolbox\\lib\\site-packages (from transformers) (0.34.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\isisi\\anaconda3\\envs\\anaconda-toolbox\\lib\\site-packages (from transformers) (2.3.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\isisi\\anaconda3\\envs\\anaconda-toolbox\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\isisi\\anaconda3\\envs\\anaconda-toolbox\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\isisi\\anaconda3\\envs\\anaconda-toolbox\\lib\\site-packages (from transformers) (2025.7.34)\n",
      "Requirement already satisfied: requests in c:\\users\\isisi\\anaconda3\\envs\\anaconda-toolbox\\lib\\site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\isisi\\anaconda3\\envs\\anaconda-toolbox\\lib\\site-packages (from transformers) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\isisi\\anaconda3\\envs\\anaconda-toolbox\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\isisi\\anaconda3\\envs\\anaconda-toolbox\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.7.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\isisi\\anaconda3\\envs\\anaconda-toolbox\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\isisi\\anaconda3\\envs\\anaconda-toolbox\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\isisi\\anaconda3\\envs\\anaconda-toolbox\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\isisi\\anaconda3\\envs\\anaconda-toolbox\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\isisi\\anaconda3\\envs\\anaconda-toolbox\\lib\\site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\isisi\\anaconda3\\envs\\anaconda-toolbox\\lib\\site-packages (from requests->transformers) (2025.7.14)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#carrega as bibliotecas necess√°rias\n",
    "%pip install transformers sentencepiece tqdm\n",
    "\n",
    "import pandas as pd # Importa a biblioteca pandas para manipula√ß√£o de dados\n",
    "import re # Importa a biblioteca re para express√µes regulares\n",
    "import os # Importa a biblioteca os para intera√ß√µes com o sistema operacional\n",
    "import hashlib # Importa a biblioteca hashlib para calcular hashes de arquivos\n",
    "import csv # Importa a biblioteca csv para manipula√ß√£o de arquivos CSV\n",
    "from pathlib import Path # Importa a classe Path da biblioteca pathlib para manipula√ß√£o de caminhos de arquivos\n",
    "import tkinter as tk # Importa a biblioteca tkinter para criar interfaces gr√°ficas\n",
    "from tkinter import filedialog, messagebox, scrolledtext # Importa componentes espec√≠ficos da biblioteca tkinter para di√°logos de arquivos, mensagens e caixas de texto rol√°veis\n",
    "from openpyxl.cell.cell import ILLEGAL_CHARACTERS_RE # Importa a express√£o regular para caracteres ilegais em nomes de arquivos do openpyxl\n",
    "import torch # Importa a biblioteca torch para computa√ß√£o em tensores\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration # Importa o tokenizador e o modelo T5 da biblioteca transformers\n",
    "from datetime import datetime# Importa a classe datetime da biblioteca datetime para manipula√ß√£o de datas e horas\n",
    "from tqdm import tqdm # Importa a biblioteca tqdm para exibir barras de progresso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef747724",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extra√ß√£o e limpeza de texto Diagn√≥stico visual de duplicados e nomes corrompidos\n",
    "# === FUN√á√ïES AUXILIARES ===\n",
    "\n",
    "def calcular_hash(caminho_arquivo, limite=1024*1024):#esta etapa tem como iobjetivo calcular o hash de um arquivo para identificar duplicados\n",
    "    \"\"\"Calcula hash de uma parte do arquivo (1 MB por padr√£o) para diagn√≥stico r√°pido.\"\"\"\n",
    "    sha256 = hashlib.sha256()\n",
    "    with open(caminho_arquivo, 'rb') as f:\n",
    "        sha256.update(f.read(limite))\n",
    "    return sha256.hexdigest()\n",
    "\n",
    "def normalizar_nome(nome):#essa etapa busca corrigir nomes de arquivos que podem estar corrompidos ou com formata√ß√£o estranha\n",
    "    \"\"\"Remove repeti√ß√µes consecutivas de sufixos.\"\"\"\n",
    "    padrao = r'(txt_\\w+\\d{4})+'\n",
    "    return re.sub(padrao, lambda m: m.group(1), nome)\n",
    "\n",
    "def diagnosticar_pasta(pasta_raiz, saida_texto):\n",
    "    pasta_raiz = Path(pasta_raiz)\n",
    "    hash_map = {}\n",
    "    log_linhas = []\n",
    "    log_path = pasta_raiz / \"log_diagnostico.csv\"\n",
    "\n",
    "    saida_texto.insert(tk.END, f\"üîç Diagn√≥stico em: {pasta_raiz}\\n\")\n",
    "    saida_texto.insert(tk.END, \"Apenas leitura ‚Äî nenhum arquivo ser√° alterado.\\n\\n\")\n",
    "    saida_texto.update()\n",
    "\n",
    "    for caminho_arquivo in pasta_raiz.rglob(\"*.txt\"):\n",
    "        hash_arquivo = calcular_hash(caminho_arquivo)\n",
    "        nome_original = caminho_arquivo.name\n",
    "        caminho_pasta = caminho_arquivo.parent\n",
    "\n",
    "        # Verifica duplicidade de conte√∫do\n",
    "        if hash_arquivo in hash_map:\n",
    "            saida_texto.insert(tk.END, f\"‚ö†Ô∏è Poss√≠vel duplicado: {nome_original} (igual a {hash_map[hash_arquivo].name})\\n\")\n",
    "            log_linhas.append([\"DUPLICADO\", str(caminho_arquivo), \"Mesma hash de\", str(hash_map[hash_arquivo])])\n",
    "        else:\n",
    "            hash_map[hash_arquivo] = caminho_arquivo\n",
    "\n",
    "        # Verifica nome corrompido\n",
    "        nome_corrigido = normalizar_nome(nome_original)\n",
    "        if nome_corrigido != nome_original:\n",
    "            saida_texto.insert(tk.END, f\"üîÅ Nome suspeito: {nome_original} ‚Üí Sugerido: {nome_corrigido}\\n\")\n",
    "            log_linhas.append([\"NOME_CORROMPIDO\", str(caminho_arquivo), \"Sugerido\", nome_corrigido])\n",
    "\n",
    "    with open(log_path, mode='w', newline='', encoding='utf-8') as log_csv:\n",
    "        writer = csv.writer(log_csv)\n",
    "        writer.writerow([\"Tipo\", \"Arquivo\", \"Info\", \"Refer√™ncia/Sugest√£o\"])\n",
    "        writer.writerows(log_linhas)\n",
    "\n",
    "    saida_texto.insert(tk.END, f\"\\nüìÑ Log salvo em: {log_path}\\n\")\n",
    "    saida_texto.insert(tk.END, \"‚úÖ Diagn√≥stico conclu√≠do.\\n\")\n",
    "    saida_texto.update()\n",
    "\n",
    "# === INTERFACE GR√ÅFICA ===\n",
    "\n",
    "def iniciar_interface():\n",
    "    def escolher_pasta():\n",
    "        pasta = filedialog.askdirectory(title=\"Selecione a pasta 'corpus_final'\")\n",
    "        if pasta:\n",
    "            entrada_pasta.delete(0, tk.END)\n",
    "            entrada_pasta.insert(0, pasta)\n",
    "\n",
    "    def executar_diagnostico():\n",
    "        pasta_escolhida = entrada_pasta.get()\n",
    "        if not os.path.isdir(pasta_escolhida):\n",
    "            messagebox.showerror(\"Erro\", \"Selecione uma pasta v√°lida.\")\n",
    "            return\n",
    "        saida_texto.delete(1.0, tk.END)\n",
    "        diagnosticar_pasta(pasta_escolhida, saida_texto)\n",
    "\n",
    "    janela = tk.Tk()\n",
    "    janela.title(\"üîé Diagn√≥stico de Arquivos Duplicados e Nomes Corrompidos\")\n",
    "\n",
    "    tk.Label(janela, text=\"üìÅ Pasta Base:\").pack(pady=5)\n",
    "    entrada_pasta = tk.Entry(janela, width=60)\n",
    "    entrada_pasta.pack()\n",
    "    tk.Button(janela, text=\"Selecionar Pasta\", command=escolher_pasta).pack(pady=5)\n",
    "\n",
    "    tk.Button(janela, text=\"üîç Executar Diagn√≥stico\", command=executar_diagnostico, bg=\"blue\", fg=\"white\").pack(pady=10)\n",
    "\n",
    "    saida_texto = scrolledtext.ScrolledText(janela, width=100, height=25)\n",
    "    saida_texto.pack(padx=10, pady=10)\n",
    "\n",
    "    janela.mainloop()\n",
    "\n",
    "# Rodar\n",
    "if __name__ == \"__main__\":\n",
    "    iniciar_interface()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c83b9c",
   "metadata": {},
   "source": [
    "#  A√ß√µes de Corre√ß√£o com Base no Diagn√≥stico\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"https://img.icons8.com/fluency/96/checked--v1.png\" width=\"80\" alt=\"Corre√ß√£o\"/>\n",
    "</p>\n",
    "\n",
    "Este script executa as a√ß√µes sugeridas pelo diagn√≥stico anterior (`log_diagnostico.csv`), removendo arquivos duplicados reais e renomeando aqueles com nomes corrompidos. Ele deve ser rodado **logo ap√≥s a Etapa 4**.\n",
    "\n",
    "### O que o script faz\n",
    "\n",
    "- **L√™ o arquivo de log** gerado na etapa anterior (`log_diagnostico.csv`), que cont√©m os arquivos identificados como duplicados ou com nomes suspeitos.\n",
    "- **Remove arquivos duplicados** de fato, com base no conte√∫do (hash), mantendo apenas uma c√≥pia.\n",
    "- **Renomeia arquivos** com nomes corrompidos, normalizando sufixos repetidos. Se o nome sugerido j√° existir, o script adiciona um sufixo incremental (`_1`, `_2`, etc.) para evitar sobrescrita.\n",
    "- **Gera um novo log (`log_execucao.csv`)**, registrando todas as a√ß√µes realizadas ou ignoradas (com justificativa).\n",
    "\n",
    "### Detalhes t√©cnicos\n",
    "\n",
    "- Os caminhos dos arquivos s√£o tratados com `Path` para garantir compatibilidade e seguran√ßa.\n",
    "- Em caso de erro (como falha ao excluir ou renomear), o erro √© capturado e registrado no log de execu√ß√£o.\n",
    "- O script imprime no terminal as a√ß√µes realizadas para acompanhamento visual.\n",
    "\n",
    "### Exemplo de a√ß√µes realizadas\n",
    "\n",
    "| Tipo           | Descri√ß√£o                                               |\n",
    "|----------------|----------------------------------------------------------|\n",
    "| REMOVIDO       | Arquivo duplicado foi deletado                          |\n",
    "| RENOMEADO      | Arquivo com nome corrompido foi renomeado corretamente  |\n",
    "| IGNORADO       | Arquivo n√£o encontrado no caminho indicado              |\n",
    "| ERRO           | Algum erro impediu a a√ß√£o (detalhes no log)             |\n",
    "\n",
    "Esta etapa complementa o diagn√≥stico anterior ao aplicar efetivamente as corre√ß√µes necess√°rias na pasta `corpus_final`, preparando os arquivos para uso limpo e padronizado no projeto IAVS.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc55887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Execu√ß√£o conclu√≠da. Log salvo em: C:\\Users\\isisi\\Documents\\IAVS_PROJETO\\corpus_final\\log_execucao.csv\n"
     ]
    }
   ],
   "source": [
    "#executar_log_diagnostico remove duplicatas reais e renomeia arquivos com nome corrompido.Logo ap√≥s o diagn√≥stico\n",
    "\n",
    "# === CONFIGURA√á√ÉO ===\n",
    "PASTA_BASE = Path(r\"C:\\Users\\isisi\\Documents\\IAVS_PROJETO\\corpus_final\")  # ajuste se necess√°rio\n",
    "CAMINHO_LOG_DIAGNOSTICO = PASTA_BASE / \"log_diagnostico.csv\"# caminho do log de diagn√≥stico\n",
    "CAMINHO_LOG_EXECUCAO = PASTA_BASE / \"log_execucao.csv\" # caminho do log de execu√ß√£o\n",
    "\n",
    "# === LEITURA DO LOG ===\n",
    "with open(CAMINHO_LOG_DIAGNOSTICO, encoding='utf-8') as f: # abre o arquivo de log de diagn√≥stico\n",
    "    leitor = csv.DictReader(f)\n",
    "    acoes = list(leitor)\n",
    "\n",
    "# === EXECU√á√ÉO DAS A√á√ïES ===\n",
    "log_execucao = [] # lista para armazenar o log de execu√ß√£o\n",
    "\n",
    "for linha in acoes: # itera sobre cada linha do log de diagn√≥stico\n",
    "    tipo = linha['Tipo']\n",
    "    caminho_str = linha['Arquivo']\n",
    "    caminho_arquivo = Path(caminho_str)\n",
    "\n",
    "    if not caminho_arquivo.exists(): # verifica se o arquivo existe\n",
    "        log_execucao.append([\"IGNORADO\", caminho_str, \"Arquivo n√£o encontrado\"])\n",
    "        continue\n",
    "\n",
    "    if tipo == \"DUPLICADO\": # se o tipo for duplicado\n",
    "        try:\n",
    "            caminho_arquivo.unlink()\n",
    "            log_execucao.append([\"REMOVIDO\", caminho_str, \"Duplicata removida\"])\n",
    "            print(f\"üóëÔ∏è Removido: {caminho_str}\")\n",
    "        except Exception as e: # tenta remover o arquivo\n",
    "            log_execucao.append([\"ERRO\", caminho_str, f\"Erro ao remover: {e}\"])\n",
    "\n",
    "    elif tipo == \"NOME_CORROMPIDO\": # se o tipo for nome corrompido\n",
    "        sugestao = linha['Refer√™ncia/Sugest√£o']\n",
    "        novo_caminho = caminho_arquivo.parent / sugestao\n",
    "        contador = 1\n",
    "        while novo_caminho.exists(): # verifica se o novo caminho j√° existe\n",
    "            novo_caminho = caminho_arquivo.parent / f\"{sugestao}_{contador}.txt\"\n",
    "            contador += 1\n",
    "        try:# tenta renomear o arquivo\n",
    "            caminho_arquivo.rename(novo_caminho)\n",
    "            log_execucao.append([\"RENOMEADO\", caminho_str, f\"Novo nome: {novo_caminho.name}\"])\n",
    "            print(f\"‚úèÔ∏è Renomeado: {caminho_arquivo.name} ‚Üí {novo_caminho.name}\")\n",
    "        except Exception as e:\n",
    "            log_execucao.append([\"ERRO\", caminho_str, f\"Erro ao renomear: {e}\"])\n",
    "\n",
    "# === SALVA LOG FINAL ===\n",
    "with open(CAMINHO_LOG_EXECUCAO, mode='w', newline='', encoding='utf-8') as f: # abre o arquivo de log de execu√ß√£o\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"A√ß√£o\", \"Arquivo\", \"Detalhes\"])\n",
    "    writer.writerows(log_execucao)\n",
    "\n",
    "print(f\"\\n‚úÖ Execu√ß√£o conclu√≠da. Log salvo em: {CAMINHO_LOG_EXECUCAO}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef8f6e2",
   "metadata": {},
   "source": [
    "# Extra√ß√£o e Valida√ß√£o de Par√°grafos V√°lidos\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"https://img.icons8.com/fluency/96/checked-checkbox.png\" width=\"80\" alt=\"Valida√ß√£o de Par√°grafos\"/>\n",
    "</p>\n",
    "\n",
    "Este script percorre todos os arquivos `.txt` na pasta `corpus_final`, extrai os par√°grafos brutos e filtra apenas os **par√°grafos considerados v√°lidos**, com o objetivo de construir uma base estruturada para an√°lise e gera√ß√£o de perguntas.\n",
    "\n",
    "### Objetivos da etapa\n",
    "\n",
    "- Quebrar corretamente os textos em par√°grafos sem ru√≠dos de quebra de linha mal formatada.\n",
    "- Eliminar blocos irrelevantes como fichas t√©cnicas, autoria, licen√ßas e se√ß√µes institucionais.\n",
    "- Salvar os par√°grafos limpos em formatos compat√≠veis com an√°lise tabular (Excel e CSV).\n",
    "\n",
    "### Crit√©rios de validade de par√°grafos\n",
    "\n",
    "Um par√°grafo √© considerado **inv√°lido** se:\n",
    "- Cont√©m menos de 5 palavras;\n",
    "- √â composto apenas por n√∫meros ou bullets (ex: \"‚Ä¢\", \"*\");\n",
    "- Cont√©m **dois ou mais termos institucionais** como \"minist√©rio\", \"licen√ßa\", \"tiragem\", \"autores\", etc.\n",
    "\n",
    "### Funcionalidades do script\n",
    "\n",
    "- `quebrar_paragrafos_brutos()`: divide o texto com base em pontua√ß√£o e letras mai√∫sculas, simulando a quebra natural de par√°grafos.\n",
    "- `paragrafo_valido()`: aplica os filtros para garantir que apenas conte√∫do relevante permane√ßa.\n",
    "- `limpar_caracteres_invalidos()`: remove s√≠mbolos que podem causar erro na exporta√ß√£o para Excel.\n",
    "\n",
    "### Arquivos gerados\n",
    "\n",
    "- **`paragrafos_validos_corpus.xlsx`**: cont√©m os par√°grafos v√°lidos organizados por arquivo e n√∫mero do par√°grafo.\n",
    "- **`paragrafos_validos_corpus.csv`**: vers√£o em CSV dos mesmos dados.\n",
    "- **`diagnostico_paragrafos_por_arquivo.xlsx`**: relat√≥rio com o total de par√°grafos lidos, quantos foram mantidos e quantos foram descartados por arquivo.\n",
    "\n",
    "### Exemplo de estrutura da sa√≠da:\n",
    "\n",
    "| Arquivo               | N√∫mero do par√°grafo | Par√°grafo                            |\n",
    "|-----------------------|---------------------|--------------------------------------|\n",
    "| dengue_guia2023.txt   | 1                   | A dengue √© uma doen√ßa viral...       |\n",
    "| dengue_guia2023.txt   | 2                   | A transmiss√£o ocorre pela picada... |\n",
    "\n",
    "| Arquivo               | Total lidos | V√°lidos mantidos | Descartados |\n",
    "|-----------------------|-------------|------------------|-------------|\n",
    "| dengue_guia2023.txt   | 150         | 87               | 63          |\n",
    "\n",
    "Essa etapa garante que o corpus textual a ser utilizado em an√°lises futuras esteja **limpo, bem segmentado e sem conte√∫dos irrelevantes**, servindo como base s√≥lida para tarefas como gera√ß√£o autom√°tica de perguntas e avalia√ß√£o de IA.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cadef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Par√°grafos v√°lidos salvos em C:\\Users\\isisi\\Documents\\IAVS_PROJETO\\corpus_final\\paragrafos_validos_corpus.xlsx\n",
      "‚úÖ 20106 par√°grafos v√°lidos salvos em C:\\Users\\isisi\\Documents\\IAVS_PROJETO\\corpus_final\\paragrafos_validos_corpus.xlsx\n",
      "ü©∫ Diagn√≥stico salvo em C:\\Users\\isisi\\Documents\\IAVS_PROJETO\\corpus_final\\diagnostico_paragrafos_por_arquivo.xlsx\n"
     ]
    }
   ],
   "source": [
    "# === CONFIGURA√á√ÉO ===\n",
    "PASTA_CORPUS = Path(r\"C:\\Users\\isisi\\Documents\\IAVS_PROJETO\\corpus_final\")\n",
    "ARQUIVO_SAIDA = PASTA_CORPUS / \"paragrafos_validos_corpus.xlsx\"\n",
    "ARQUIVO_DIAGNOSTICO = PASTA_CORPUS / \"diagnostico_paragrafos_por_arquivo.xlsx\"\n",
    "\n",
    "# === FUN√á√ïES ===\n",
    "def paragrafo_valido(paragrafo):\n",
    "    paragrafo = paragrafo.strip()\n",
    "    if len(paragrafo.split()) < 5:\n",
    "        return False\n",
    "    if re.match(r'^(\\d+|[‚Ä¢*])$', paragrafo):  # apenas n√∫mero ou bullet\n",
    "        return False\n",
    "    if sum(1 for palavra in [\"minist√©rio\", \"licen√ßa\", \"tiragem\", \"elabora√ß√£o\", \"cep\", \"autores\", \"vers√£o\", \"editora√ß√£o\"] if palavra in paragrafo.lower()) >= 2:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def limpar_caracteres_invalidos(texto):\n",
    "    return ILLEGAL_CHARACTERS_RE.sub(\"\", texto)\n",
    "\n",
    "def quebrar_paragrafos_brutos(texto):\n",
    "    # Substitui quebras de linha simples por espa√ßo (evita quebra errada)\n",
    "    texto = texto.replace(\"\\n\", \" \")\n",
    "    # Insere quebra for√ßada ap√≥s ponto final seguido de espa√ßo e letra mai√∫scula\n",
    "    texto = re.sub(r'\\. (?=[A-Z√Å√â√ç√ì√ö√á])', '.\\n\\n', texto)\n",
    "    # Divide onde houver \\n\\n for√ßado\n",
    "    return [p.strip() for p in texto.split('\\n\\n') if p.strip()]\n",
    "\n",
    "# === PROCESSAMENTO ===\n",
    "dados = []\n",
    "diagnostico = []\n",
    "\n",
    "for caminho_txt in PASTA_CORPUS.rglob(\"*.txt\"):\n",
    "    try:\n",
    "        with open(caminho_txt, \"r\", encoding=\"utf-8\") as f:\n",
    "            conteudo = f.read()\n",
    "\n",
    "        # Divide usando segmenta√ß√£o for√ßada\n",
    "        paragrafos_raw = quebrar_paragrafos_brutos(conteudo)\n",
    "        paragrafos_validos = [p for p in paragrafos_raw if paragrafo_valido(p)]\n",
    "\n",
    "        for i, paragrafo in enumerate(paragrafos_validos, 1):\n",
    "            dados.append({\n",
    "                \"Arquivo\": caminho_txt.name,\n",
    "                \"Par√°grafo\": limpar_caracteres_invalidos(paragrafo),\n",
    "                \"N√∫mero do par√°grafo\": i\n",
    "            })\n",
    "\n",
    "        diagnostico.append({\n",
    "            \"Arquivo\": caminho_txt.name,\n",
    "            \"Total lidos\": len(paragrafos_raw),\n",
    "            \"V√°lidos mantidos\": len(paragrafos_validos),\n",
    "            \"Descartados\": len(paragrafos_raw) - len(paragrafos_validos)\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Erro ao processar {caminho_txt.name}: {e}\")\n",
    "\n",
    "# === EXPORTA√á√ÉO ===\n",
    "df_paragrafos = pd.DataFrame(dados)\n",
    "df_paragrafos.to_excel(ARQUIVO_SAIDA, index=False)\n",
    "\n",
    "df_diag = pd.DataFrame(diagnostico)\n",
    "df_diag.to_excel(ARQUIVO_DIAGNOSTICO, index=False)\n",
    "\n",
    "ARQUIVO_CSV = PASTA_CORPUS / \"paragrafos_validos_corpus.csv\"\n",
    "df_paragrafos.to_csv(ARQUIVO_CSV, index=False, encoding=\"utf-8\")\n",
    "print(f\"üìÑ Par√°grafos v√°lidos salvos em {ARQUIVO_SAIDA}\")\n",
    "print(f\"‚úÖ {len(df_paragrafos)} par√°grafos v√°lidos salvos em {ARQUIVO_SAIDA}\")\n",
    "print(f\"ü©∫ Diagn√≥stico salvo em {ARQUIVO_DIAGNOSTICO}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bbf239",
   "metadata": {},
   "source": [
    "# Consolida√ß√£o e Resumo dos Arquivos √önicos (.CSV)\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"https://img.icons8.com/ios-filled/100/document--v1.png\" width=\"80\" alt=\"Arquivos √önicos\"/>\n",
    "</p>\n",
    "\n",
    "Esta etapa realiza a consolida√ß√£o e an√°lise dos arquivos presentes em `corpus_final`, utilizando o arquivo de entrada `resumo_corpus_final.csv`, que cont√©m metadados como tamanho, n√∫mero de linhas e caracteres por arquivo.\n",
    "\n",
    "### Objetivos da etapa\n",
    "\n",
    "- **Identificar e manter apenas arquivos √∫nicos**, mesmo quando h√° nomes duplicados ou corrompidos (ex: `dengue_txttxt.txt`).\n",
    "- **Agrupar e sumarizar por subpasta**, fornecendo uma vis√£o geral da distribui√ß√£o de documentos por tema ou doen√ßa.\n",
    "\n",
    "### O que o script faz\n",
    "\n",
    "- **Importa o CSV de entrada** com os metadados de todos os arquivos `.txt`.\n",
    "- **Normaliza os nomes de arquivo**, criando a coluna `Arquivo_base` para eliminar duplica√ß√µes.\n",
    "- **Filtra os arquivos √∫nicos** por subpasta e nome base.\n",
    "- **Agrupa por subpasta**, calculando:\n",
    "  - Total de arquivos √∫nicos\n",
    "  - Tamanho total (bytes)\n",
    "  - M√©dia de linhas por arquivo\n",
    "  - M√©dia de caracteres por arquivo\n",
    "\n",
    "### Arquivos gerados\n",
    "\n",
    "- `resumo_arquivos_unicos.csv`: lista dos arquivos √∫nicos por subpasta.\n",
    "- `resumo_por_subpasta.csv`: tabela de resumo consolidado por subpasta.\n",
    "\n",
    "| Subpasta    | Total de arquivos √∫nicos | Tamanho total (bytes) | M√©dia de linhas | M√©dia de caracteres |\n",
    "|-------------|---------------------------|------------------------|------------------|----------------------|\n",
    "| dengue      | 8                         | 153000                 | 117              | 5300                 |\n",
    "| tuberculose | 6                         | 94000                  | 98               | 4100                 |\n",
    "\n",
    "### Observa√ß√µes\n",
    "\n",
    "- A deduplica√ß√£o considera tanto o nome quanto a subpasta.\n",
    "- Os arquivos finais est√£o em formato `.csv`, permitindo f√°cil importa√ß√£o por sistemas externos, planilhas e pain√©is.\n",
    "- Esta etapa depende da consist√™ncia dos nomes gerados nas etapas anteriores (corre√ß√£o e diagn√≥stico).\n",
    "\n",
    "O resultado √© um panorama claro e confi√°vel da composi√ß√£o do corpus final, essencial para valida√ß√µes e planejamentos posteriores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3879a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 39\n",
      "‚úÖ Arquivo √∫nico salvo em: C:\\Users\\isisi\\Documents\\IAVS_PROJETO\\corpus_final\\resumo_arquivos_unicos.xlsx\n",
      "üìä Resumo por subpasta salvo em: C:\\Users\\isisi\\Documents\\IAVS_PROJETO\\corpus_final\\resumo_por_subpasta.xlsx\n"
     ]
    }
   ],
   "source": [
    "# === CONFIGURA√á√ÉO ===\n",
    "PASTA_BASE = Path(r\"C:\\Users\\isisi\\Documents\\IAVS_PROJETO\\corpus_final\") #identifica a pasta base onde os arquivos est√£o localizados\n",
    "ARQUIVO_ENTRADA = PASTA_BASE / \"resumo_corpus_final.csv\"# caminho do arquivo CSV de entrada com o resumo dos arquivos\n",
    "ARQUIVO_UNICOS = PASTA_BASE / \"resumo_arquivos_unicos.csv\"# caminho do arquivo de sa√≠da com os arquivos √∫nicos\n",
    "ARQUIVO_AGREGADO = PASTA_BASE / \"resumo_por_subpasta.csv\" # caminho do arquivo de sa√≠da com o resumo por subpasta\n",
    "\n",
    "# === LEITURA DO ARQUIVO DE RESUMO ===\n",
    "df = pd.read_csv(ARQUIVO_ENTRADA)\n",
    "\n",
    "# === REMOVER DUPLICATAS POR NOME BASE DO ARQUIVO ===\n",
    "# Extrai o nome base (sem partes repetidas)\n",
    "df[\"Arquivo_base\"] = df[\"Arquivo\"].str.extract(r\"^(.+?)(?:_?txt)+(?:\\.txt)?$\", expand=False).str.strip()#aqui, usamos uma express√£o regular para extrair o nome base do arquivo, removendo partes repetidas como \"_txt\" ou \".txt\". Isso ajuda a identificar arquivos com nomes semelhantes.\n",
    "df_unicos = df.drop_duplicates(subset=[\"Subpasta\", \"Arquivo_base\"])#j√° aqui, removemos duplicatas com base na subpasta e no nome base do arquivo. Isso garante que apenas um arquivo por subpasta e nome base seja mantido.\n",
    "\n",
    "# === AGRUPAMENTO POR SUBPASTA ===\n",
    "# Agrupa os dados por subpasta e calcula as estat√≠sticas desejadas\n",
    "df_grupo = df_unicos.groupby(\"Subpasta\").agg({\n",
    "    \"Arquivo\": \"count\",\n",
    "    \"Tamanho (bytes)\": \"sum\",\n",
    "    \"N¬∫ de linhas\": \"mean\",\n",
    "    \"N¬∫ de caracteres\": \"mean\"\n",
    "}).reset_index()\n",
    "\n",
    "df_grupo = df_grupo.rename(columns={\n",
    "    \"Arquivo\": \"Total de arquivos √∫nicos\",\n",
    "    \"Tamanho (bytes)\": \"Tamanho total (bytes)\",\n",
    "    \"N¬∫ de linhas\": \"M√©dia de linhas\",\n",
    "    \"N¬∫ de caracteres\": \"M√©dia de caracteres\"\n",
    "})\n",
    "\n",
    "# === EXPORTA√á√ÉO ===\n",
    "# (Removido: df_unicos.to_excel(ARQUIVO_UNICOS, index=False))\n",
    "df_unicos.to_csv(ARQUIVO_UNICOS, index=False, encoding=\"utf-8\")\n",
    "df_grupo.to_csv(ARQUIVO_AGREGADO, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"‚úÖ Arquivo √∫nico salvo em: {ARQUIVO_UNICOS}\")\n",
    "print(f\"üìä Resumo por subpasta salvo em: {ARQUIVO_AGREGADO}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee86fa1",
   "metadata": {},
   "source": [
    "# Gera√ß√£o do Corpus Unificado (`corpus_unificado.txt`)\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://img.icons8.com/ios-filled/100/idea.png\" width=\"80\" alt=\"L√¢mpada - Ideia\"/>\n",
    "</p>\n",
    "\n",
    "Ap√≥s as etapas de extra√ß√£o, limpeza, deduplica√ß√£o e valida√ß√£o dos arquivos `.txt`, esta etapa consolida todo o conte√∫do textual da pasta `corpus_final` em um √∫nico arquivo chamado `corpus_unificado.txt`.\n",
    "\n",
    "### Objetivos da etapa\n",
    "\n",
    "- **Agrupar todos os arquivos limpos** em um √∫nico arquivo de texto.\n",
    "- **Preservar a estrutura organizacional** dos documentos, mantendo a identifica√ß√£o por subpasta (tema/doen√ßa) e nome do arquivo.\n",
    "- **Facilitar a inspe√ß√£o visual**, revis√£o manual ou busca textual do corpus consolidado.\n",
    "\n",
    "### O que o script faz\n",
    "\n",
    "- Percorre recursivamente todos os arquivos `.txt` da pasta `corpus_final`.\n",
    "- Ignora arquivos muito curtos (menos de 50 caracteres), que provavelmente s√£o inv√°lidos ou vazios.\n",
    "- Para cada arquivo v√°lido:\n",
    "  - L√™ o conte√∫do e remove espa√ßos em branco no in√≠cio e fim.\n",
    "  - Identifica o nome do arquivo e a subpasta a que pertence.\n",
    "  - Adiciona ao `corpus_unificado.txt` com um cabe√ßalho no formato:\n",
    "\n",
    "    ### Arquivo: nome_do_arquivo.txt | Subpasta: nome_da_subpasta\n",
    "    ```\n",
    "\n",
    "- Salva o resultado em `corpus_final/corpus_unificado.txt`.\n",
    "\n",
    "### Finalidade do arquivo `corpus_unificado.txt`\n",
    "\n",
    "- Permite uma **valida√ß√£o r√°pida do conte√∫do final** do corpus.\n",
    "- Pode ser aberto em qualquer editor de texto para:\n",
    "  - Revis√£o por especialistas;\n",
    "  - Busca de termos relevantes;\n",
    "  - Confer√™ncia de organiza√ß√£o e integridade textual;\n",
    "  - Gera√ß√£o de √≠ndices ou relat√≥rios.\n",
    "\n",
    "### Exemplo de estrutura no arquivo final\n",
    "\n",
    "Arquivo: dengue_transmissao.txt | Subpasta: dengue\n",
    "A dengue √© uma doen√ßa viral transmitida principalmente por mosquitos do g√™nero Aedes...\n",
    "\n",
    "Arquivo: hanseniase_tratamento.txt | Subpasta: hanseniase\n",
    "O tratamento da hansen√≠ase √© ofertado gratuitamente pelo SUS e baseado na poliquimioterapia...\n",
    "\n",
    "\n",
    "### Observa√ß√µes\n",
    "\n",
    "- O script pode ser executado sempre que houver atualiza√ß√£o nos textos limpos.\n",
    "- Ele n√£o altera os arquivos originais, apenas l√™ e concatena.\n",
    "- √ötil para revisar o corpus como um todo sem precisar abrir os arquivos individualmente.\n",
    "\n",
    "Esta etapa finaliza a constru√ß√£o do corpus textual unificado, sendo um ponto de partida para processos de an√°lise autom√°tica, revis√£o qualitativa ou auditorias de conte√∫do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82afd0f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Corpus unificado salvo em: C:\\Users\\isisi\\Documents\\IAVS_PROJETO\\corpus_final\\corpus_unificado.txt\n"
     ]
    }
   ],
   "source": [
    "# === CONFIGURA√á√ïES ===\n",
    "PASTA_CORPUS = Path(r\"C:\\Users\\isisi\\Documents\\IAVS_PROJETO\\corpus_final\")# identifica a pasta base onde os arquivos est√£o localizados\n",
    "ARQUIVO_SAIDA = PASTA_CORPUS / \"corpus_unificado.txt\"# caminho do arquivo de sa√≠da onde o corpus unificado ser√° salvo\n",
    "\n",
    "# === UNIFICAR CONTE√öDO ===\n",
    "with open(ARQUIVO_SAIDA, \"w\", encoding=\"utf-8\") as arquivo_saida: # abre o arquivo de sa√≠da para escrita\n",
    "    for caminho_txt in PASTA_CORPUS.rglob(\"*.txt\"):\n",
    "        try:\n",
    "            with open(caminho_txt, \"r\", encoding=\"utf-8\") as f:\n",
    "                conteudo = f.read().strip()\n",
    "\n",
    "            if len(conteudo) < 50:\n",
    "                continue  # ignora arquivos muito curtos\n",
    "\n",
    "            nome_arquivo = caminho_txt.name\n",
    "            subpasta = caminho_txt.parent.name\n",
    "\n",
    "            bloco = f\"\\n\\n### Arquivo: {nome_arquivo} | Subpasta: {subpasta}\\n\\n{conteudo}\\n\\n\"\n",
    "            arquivo_saida.write(bloco)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao ler {caminho_txt.name}: {e}\") # ignora erros de leitura\n",
    "\n",
    "print(f\"‚úÖ Corpus unificado salvo em: {ARQUIVO_SAIDA}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff10d7a",
   "metadata": {},
   "source": [
    "# <img src=\"https://img.icons8.com/ios-filled/100/document--v1.png\" width=\"40\" style=\"vertical-align:middle; margin-right:8px;\"/> Filtragem Sem√¢ntica R√≠gida dos Par√°grafos\n",
    "\n",
    "\n",
    "Esta etapa aplica uma filtragem sem√¢ntica rigorosa ao conjunto de par√°grafos v√°lidos previamente extra√≠dos, com o objetivo de manter **apenas os trechos realmente relevantes para an√°lise textual ou treinamento de modelos de IA**.\n",
    "\n",
    "### Objetivos da etapa\n",
    "\n",
    "- **Descartar par√°grafos institucionais ou t√©cnicos irrelevantes**, como autoria, endere√ßo, ficha catalogr√°fica e licenciamento.\n",
    "- **Evitar par√°grafos com conte√∫do residual**, como listas de anexo, sum√°rio ou campos vazios.\n",
    "- **Aplicar um filtro mais protetivo para seguran√ßa e qualidade** do corpus final.\n",
    "\n",
    "### Funcionalidades do script\n",
    "\n",
    "- **Leitura do arquivo `paragrafos_validos_corpus.csv`**, que cont√©m todos os par√°grafos classificados como v√°lidos em etapas anteriores.\n",
    "- **Aplica√ß√£o de filtros baseados em regras lingu√≠sticas e estruturais**, incluindo:\n",
    "  - Tamanho m√°ximo de 3000 caracteres.\n",
    "  - Exclus√£o por presen√ßa de padr√µes indesej√°veis (ex: ‚Äúnome do autor‚Äù, ‚Äúwww.‚Äù, ‚Äúminist√©rio da sa√∫de‚Äù, ‚Äútiragem‚Äù, ‚Äú√≠ndice‚Äù, etc.).\n",
    "  - Exclus√£o de par√°grafos com menos de 6 palavras.\n",
    "\n",
    "- **Separa√ß√£o dos dados** em dois conjuntos:\n",
    "  - `paragrafos_filtrados_corpus.csv`: apenas os par√°grafos considerados relevantes.\n",
    "  - `paragrafos_descartados.csv`: par√°grafos eliminados por n√£o atenderem aos crit√©rios.\n",
    "\n",
    "- **Gera√ß√£o de um relat√≥rio quantitativo (`relatorio_filtragem_paragrafos.csv`)** com estat√≠sticas resumidas do processo de filtragem.\n",
    "\n",
    "### Exemplo de padr√µes que s√£o filtrados\n",
    "\n",
    "| Tipo de conte√∫do exclu√≠do           | Exemplo de padr√£o identificado                     |\n",
    "|-------------------------------------|----------------------------------------------------|\n",
    "| Identifica√ß√£o institucional         | `Minist√©rio da Sa√∫de`, `Coordena√ß√£o-Geral`        |\n",
    "| Autoria ou ficha t√©cnica            | `Autores:`, `Revis√£o:`, `Nome do autor:`          |\n",
    "| Informa√ß√µes log√≠sticas e endere√ßo  | `CEP:`, `Quadra`, `www.saude.gov.br`              |\n",
    "| Refer√™ncias editoriais             | `Esta obra √© licenciada...`, `Edi√ß√£o 2023`        |\n",
    "| T√≥picos n√£o informativos           | `Anexo`, `√çndice`, `Ap√™ndice`                     |\n",
    "\n",
    "### Arquivos gerados\n",
    "\n",
    "- **`paragrafos_filtrados_corpus.csv`** ‚Äì Corpus final com apenas par√°grafos relevantes.\n",
    "- **`paragrafos_descartados.csv`** ‚Äì Arquivo com todos os par√°grafos exclu√≠dos na filtragem.\n",
    "- **`relatorio_filtragem_paragrafos.csv`** ‚Äì Relat√≥rio estat√≠stico com o total de par√°grafos, mantidos, exclu√≠dos e suas respectivas propor√ß√µes.\n",
    "\n",
    "| M√©trica         | Valor     |\n",
    "|-----------------|-----------|\n",
    "| Total original  | 12.000    |\n",
    "| Total mantido   | 7.400     |\n",
    "| Total exclu√≠do  | 4.600     |\n",
    "| % mantido       | 61.67%    |\n",
    "| % exclu√≠do      | 38.33%    |\n",
    "\n",
    "### Observa√ß√µes\n",
    "\n",
    "- O filtro √© propositalmente conservador, priorizando precis√£o em detrimento de abrang√™ncia.\n",
    "- Os crit√©rios podem ser ajustados posteriormente conforme o feedback da equipe t√©cnica ou dos especialistas em conte√∫do.\n",
    "\n",
    "Esta etapa garante que o corpus final esteja limpo, consistente e focado no conte√∫do t√©cnico-informativo, evitando ru√≠dos que comprometam a qualidade da an√°lise ou da aplica√ß√£o de intelig√™ncia artificial.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e19876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ CSV salvo com 17640 par√°grafos: C:\\Users\\isisi\\Documents\\IAVS_PROJETO\\corpus_final\\paragrafos_filtrados_corpus.csv\n",
      "üìä Relat√≥rio salvo: C:\\Users\\isisi\\Documents\\IAVS_PROJETO\\corpus_final\\relatorio_filtragem_paragrafos.csv\n"
     ]
    }
   ],
   "source": [
    "# === CAMINHOS ===\n",
    "PASTA = r\"C:\\Users\\isisi\\Documents\\IAVS_PROJETO\\corpus_final\" # identifica a pasta base onde os arquivos est√£o localizados\n",
    "ARQUIVO_ENTRADA = os.path.join(PASTA, \"paragrafos_validos_corpus.csv\") # caminho do arquivo CSV de entrada com os par√°grafos v√°lidos\n",
    "ARQUIVO_SAIDA = os.path.join(PASTA, \"paragrafos_filtrados_corpus.csv\") # caminho do arquivo CSV de sa√≠da com os par√°grafos filtrados\n",
    "ARQUIVO_RELATORIO = os.path.join(PASTA, \"relatorio_filtragem_paragrafos.csv\") # caminho do arquivo de relat√≥rio com estat√≠sticas da filtragem\n",
    "\n",
    "# === FILTRO SEM√ÇNTICO COM PROTE√á√ïES R√çGIDAS ===\n",
    "def paragrafo_relevante(texto):\n",
    "    texto_limpo = str(texto).strip()\n",
    "\n",
    "    if len(texto_limpo) > 3000:  # ainda mais seguro\n",
    "        return False\n",
    "    if ILLEGAL_CHARACTERS_RE.search(texto_limpo):\n",
    "        return False\n",
    "\n",
    "    texto_limpo = texto_limpo.lower()\n",
    "    padroes_excluir = [\n",
    "        r'nomes?[:\\-]',\n",
    "        r'e-mail|site:|www\\.',\n",
    "        r'cep:|quadra|srtv|endere',\n",
    "        r'coordena√ß√£o-geral|departamento',\n",
    "        r'minist√©rio da sa√∫de',\n",
    "        r'esta obra.*licen√ßa',\n",
    "        r'(nome|cargo) do autor',\n",
    "        r'sa√∫de p√∫blica brasileira.*acesso',\n",
    "        r'\\b(anexo|ap√™ndice|√≠ndice)\\b',\n",
    "        r'(autores?|vers√£o|tiragem|edi√ß√£o).*202[0-9]',\n",
    "        r'(sigla|significado) de ',\n",
    "        r'(organizador|respons√°vel|revis√£o)',\n",
    "    ]\n",
    "    for padrao in padroes_excluir:\n",
    "        if re.search(padrao, texto_limpo):\n",
    "            return False\n",
    "    if len(texto_limpo.split()) < 6:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# === LEITURA ===\n",
    "df = pd.read_csv(ARQUIVO_ENTRADA)\n",
    "df[\"Relevante\"] = df[\"Par√°grafo\"].astype(str).apply(paragrafo_relevante)\n",
    "\n",
    "# === SEPARA√á√ÉO ===\n",
    "df_filtrado = df[df[\"Relevante\"]].drop(columns=[\"Relevante\"])\n",
    "df_excluido = df[~df[\"Relevante\"]].drop(columns=[\"Relevante\"])\n",
    "\n",
    "# === SALVAR RESULTADOS EM .CSV ===\n",
    "df_filtrado.to_csv(ARQUIVO_SAIDA, index=False, encoding=\"utf-8\")\n",
    "df_excluido.to_csv(PASTA + \"/paragrafos_descartados.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "# === RELAT√ìRIO RESUMIDO ===\n",
    "resumo = pd.DataFrame([{\n",
    "    \"Total original\": len(df),\n",
    "    \"Total mantido\": len(df_filtrado),\n",
    "    \"Total exclu√≠do\": len(df_excluido),\n",
    "    \"% mantido\": round(100 * len(df_filtrado) / len(df), 2),\n",
    "    \"% exclu√≠do\": round(100 * len(df_excluido) / len(df), 2),\n",
    "}])\n",
    "resumo.to_csv(ARQUIVO_RELATORIO, index=False)\n",
    "\n",
    "print(f\"‚úÖ CSV salvo com {len(df_filtrado)} par√°grafos: {ARQUIVO_SAIDA}\")\n",
    "print(f\"üìä Relat√≥rio salvo: {ARQUIVO_RELATORIO}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757523d5",
   "metadata": {},
   "source": [
    "# <img src=\"https://img.icons8.com/ios-filled/100/ask-question.png\" width=\"40\" style=\"vertical-align:middle; margin-right:8px;\"/> Gera√ß√£o de Perguntas com Modelo T5\n",
    "\n",
    "Esta etapa aplica **gera√ß√£o autom√°tica de perguntas** a partir dos par√°grafos previamente filtrados, utilizando o modelo de linguagem `valhalla/t5-base-qg-hl`. O objetivo √© criar uma base de quest√µes que poder√° ser usada para treinamento, avalia√ß√£o de IA ou an√°lise de compreens√£o textual.\n",
    "\n",
    "O **T5 (Text-to-Text Transfer Transformer)** √© um modelo de linguagem desenvolvido pelo Google que converte todas as tarefas de processamento de linguagem natural em um formato de entrada e sa√≠da de texto para texto. Isso significa que, para tarefas como tradu√ß√£o, resumo, resposta a perguntas ou gera√ß√£o de perguntas, o T5 recebe uma instru√ß√£o textual e retorna uma resposta tamb√©m em texto. Sua arquitetura baseada em transformers permite grande flexibilidade e desempenho em m√∫ltiplas tarefas, sendo amplamente utilizado para aplica√ß√µes avan√ßadas de NLP.\n",
    "\n",
    "### Objetivos da etapa\n",
    "\n",
    "- Gerar uma pergunta para cada par√°grafo considerado relevante no corpus.\n",
    "- Dividir o processo em blocos para **evitar sobrecarga de mem√≥ria e facilitar retomadas**.\n",
    "- Salvar os resultados em arquivos separados por lote.\n",
    "\n",
    "### Funcionalidades do script\n",
    "\n",
    "- **Carregamento do modelo T5** e tokenizer (`valhalla/t5-base-qg-hl`) com suporte a GPU (caso dispon√≠vel).\n",
    "- **Leitura do arquivo `paragrafos_filtrados_corpus.csv`**, contendo os textos-base para gera√ß√£o.\n",
    "- **Divis√£o do corpus em blocos de 1000 par√°grafos**, controlando o tamanho dos lotes para evitar falhas em m√°quinas com menos mem√≥ria.\n",
    "- **Gera√ß√£o de perguntas** com base na instru√ß√£o `generate question: <texto>`, em que o modelo entende que deve formular uma pergunta sobre o conte√∫do apresentado.\n",
    "- **Exporta√ß√£o dos blocos** em arquivos CSV nomeados sequencialmente, por exemplo: `perguntas_001.csv`, `perguntas_002.csv`, etc.\n",
    "- **Verifica√ß√£o autom√°tica de blocos j√° processados**, evitando retrabalho em execu√ß√µes futuras.\n",
    "\n",
    "### Estrutura do arquivo gerado por bloco\n",
    "\n",
    "| Par√°grafo                                                     | Pergunta                                      |\n",
    "|---------------------------------------------------------------|-----------------------------------------------|\n",
    "| A dengue √© uma doen√ßa viral transmitida por mosquitos...      | Qual √© o modo de transmiss√£o da dengue?       |\n",
    "| O tratamento da tuberculose dura no m√≠nimo seis meses...      | Quanto tempo dura o tratamento da tuberculose?|\n",
    "\n",
    "### Organiza√ß√£o dos resultados\n",
    "\n",
    "- As perguntas s√£o salvas em uma subpasta com data do dia de execu√ß√£o, ex:  \n",
    "  `corpus_final/perguntas_geradas_20250803/`\n",
    "- Cada arquivo cont√©m at√© 1000 pares de par√°grafo/pergunta.\n",
    "- A pasta de sa√≠da √© criada automaticamente, se ainda n√£o existir.\n",
    "\n",
    "### Observa√ß√µes\n",
    "\n",
    "- A gera√ß√£o √© feita com **m√°ximo de 64 tokens** por pergunta para garantir objetividade.\n",
    "- O script pode ser executado novamente sem sobrescrever resultados j√° salvos, tornando-o seguro para execu√ß√µes parciais.\n",
    "- O modelo `valhalla/t5-base-qg-hl` foi escolhido por seu bom desempenho em tarefas de question generation em portugu√™s e ingl√™s, mas pode ser substitu√≠do se necess√°rio.\n",
    "\n",
    "Essa etapa transforma o corpus textual em uma base estruturada de perguntas, pronta para ser usada em avalia√ß√µes manuais, benchmarking de IA, ou sistemas de apoio ao ensino e treinamento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5101c328",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade torch --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12ae1e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≠Ô∏è Bloco 1 j√° processado: C:\\Users\\isisi\\Documents\\IAVS_PROJETO\\corpus_final\\perguntas_geradas_20250804\\perguntas_001.csv\n",
      "‚è≠Ô∏è Bloco 2 j√° processado: C:\\Users\\isisi\\Documents\\IAVS_PROJETO\\corpus_final\\perguntas_geradas_20250804\\perguntas_002.csv\n",
      "‚è≠Ô∏è Bloco 3 j√° processado: C:\\Users\\isisi\\Documents\\IAVS_PROJETO\\corpus_final\\perguntas_geradas_20250804\\perguntas_003.csv\n",
      "‚è≠Ô∏è Bloco 4 j√° processado: C:\\Users\\isisi\\Documents\\IAVS_PROJETO\\corpus_final\\perguntas_geradas_20250804\\perguntas_004.csv\n",
      "‚è≠Ô∏è Bloco 5 j√° processado: C:\\Users\\isisi\\Documents\\IAVS_PROJETO\\corpus_final\\perguntas_geradas_20250804\\perguntas_005.csv\n",
      "‚è≠Ô∏è Bloco 6 j√° processado: C:\\Users\\isisi\\Documents\\IAVS_PROJETO\\corpus_final\\perguntas_geradas_20250804\\perguntas_006.csv\n",
      "‚è≠Ô∏è Bloco 7 j√° processado: C:\\Users\\isisi\\Documents\\IAVS_PROJETO\\corpus_final\\perguntas_geradas_20250804\\perguntas_007.csv\n",
      "‚è≠Ô∏è Bloco 8 j√° processado: C:\\Users\\isisi\\Documents\\IAVS_PROJETO\\corpus_final\\perguntas_geradas_20250804\\perguntas_008.csv\n",
      "‚è≠Ô∏è Bloco 9 j√° processado: C:\\Users\\isisi\\Documents\\IAVS_PROJETO\\corpus_final\\perguntas_geradas_20250804\\perguntas_009.csv\n",
      "‚è≠Ô∏è Bloco 10 j√° processado: C:\\Users\\isisi\\Documents\\IAVS_PROJETO\\corpus_final\\perguntas_geradas_20250804\\perguntas_010.csv\n",
      "üîÑ Gerando perguntas para bloco 11 (10000‚Äì11000)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bloco 11:   4%|‚ñé         | 36/1000 [00:13<04:33,  3.52it/s]"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# === CONFIGURA√á√ïES ===\n",
    "PASTA = r\"C:\\Users\\isisi\\Documents\\IAVS_PROJETO\\corpus_final\" # Identifica a pasta base onde os arquivos est√£o localizados\n",
    "ARQUIVO_ENTRADA = os.path.join(PASTA, \"paragrafos_filtrados_corpus.csv\") # Caminho do arquivo CSV de entrada com os par√°grafos filtrados\n",
    "DATA = datetime.today().strftime('%Y%m%d') # Obt√©m a data atual no formato YYYYMMDD\n",
    "PASTA_SAIDA = os.path.join(PASTA, f\"perguntas_geradas_{DATA}\")# Caminho da pasta de sa√≠da onde as perguntas geradas ser√£o salvas\n",
    "os.makedirs(PASTA_SAIDA, exist_ok=True)# Cria a pasta de sa√≠da se n√£o existir\n",
    "\n",
    "BLOCO_TAMANHO = 1000 #define o tamanho do bloco de processamento, ou seja, quantas linhas ser√£o processadas por vez\n",
    "\n",
    "# === MODELO DE PERGUNTAS ===\n",
    "modelo = \"mrm8488/t5-base-finetuned-question-generation-ap\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(modelo) \n",
    "model = T5ForConditionalGeneration.from_pretrained(modelo) \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device) \n",
    "\n",
    "# === FUN√á√ÉO DE GERA√á√ÉO ===\n",
    "def gerar_pergunta(texto):\n",
    "    entrada = f\"generate question: {texto}\"\n",
    "    inputs = tokenizer(entrada, return_tensors=\"pt\", truncation=True).to(device)\n",
    "    outputs = model.generate(inputs[\"input_ids\"], max_length=64)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# === CARREGAR DADOS ===\n",
    "df = pd.read_csv(ARQUIVO_ENTRADA).reset_index(drop=True)\n",
    "total_linhas = len(df)\n",
    "num_blocos = (total_linhas + BLOCO_TAMANHO - 1) // BLOCO_TAMANHO  # Arredonda para cima\n",
    "\n",
    "# === PROCESSAMENTO POR BLOCO ===\n",
    "# === PROCESSAMENTO POR BLOCO ===\n",
    "for i in range(num_blocos):\n",
    "    inicio_tempo = time.time()\n",
    "    inicio = i * BLOCO_TAMANHO\n",
    "    fim = min((i + 1) * BLOCO_TAMANHO, total_linhas)\n",
    "    bloco_df = df.iloc[inicio:fim].copy()\n",
    "    saida_parcial = os.path.join(PASTA_SAIDA, f\"perguntas_{i+1:03}.csv\")\n",
    "\n",
    "    # Libera mem√≥ria da GPU ANTES\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Pular se j√° existir\n",
    "    if os.path.exists(saida_parcial):\n",
    "        print(f\"‚è≠Ô∏è Bloco {i+1} j√° processado: {saida_parcial}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"üîÑ Gerando perguntas para bloco {i+1} ({inicio}‚Äì{fim})...\")\n",
    "\n",
    "    perguntas = []\n",
    "    for paragrafo in tqdm(bloco_df[\"Par√°grafo\"].astype(str), desc=f\"Bloco {i+1}\"):\n",
    "        try:\n",
    "            pergunta = gerar_pergunta(paragrafo)\n",
    "        except Exception as e:\n",
    "            pergunta = f\"[Erro: {str(e)}]\"\n",
    "        perguntas.append(pergunta)\n",
    "\n",
    "        # Libera GPU entre cada gera√ß√£o, se necess√°rio\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    fim_tempo = time.time()\n",
    "    print(f\"‚è±Ô∏è Tempo gasto no bloco {i+1}: {fim_tempo - inicio_tempo:.2f} segundos\")\n",
    "\n",
    "    bloco_df[\"Pergunta\"] = perguntas\n",
    "    bloco_df.to_csv(saida_parcial, index=False, encoding=\"utf-8\")\n",
    "    print(f\"‚úÖ Perguntas salvas em: {saida_parcial}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-toolbox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
